{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "1. Data is clearned and not errorneous\n",
    "2. Timezone consideration is not required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.time.temporal.ChronoUnit\n",
    "import java.time.{Period, LocalDate, Instant}\n",
    "import java.sql.Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parition control based on core availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NUM_CORES = 4\n",
       "NUM_PARTITIONS = 3\n",
       "spark = <lazy>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<lazy>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NUM_CORES = 4\n",
    "val NUM_PARTITIONS = 3\n",
    "\n",
    "lazy val spark: SparkSession = SparkSession.builder()\n",
    "    .master(\"local\")\n",
    "    .appName(\"flight\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FLIGHTDATA_CSV_PATH = ../resources/flightData.csv\n",
       "PASSENGER_CSV_PATH = ../resources/passengers.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "../resources/passengers.csv"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val FLIGHTDATA_CSV_PATH = \"../resources/flightData.csv\"\n",
    "val PASSENGER_CSV_PATH = \"../resources/passengers.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timing = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "timed: [T](label: String, code: => T)T\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val timing = new StringBuffer\n",
    "def timed[T](label: String, code: => T): T = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val result = code\n",
    "    val stop = System.currentTimeMillis()\n",
    "    timing.append(s\"Processing $label took ${stop - start} ms.\\n\")\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:78: error: missing argument list for method timed\n",
       "Unapplied methods are only converted to functions when a function type is expected.\n",
       "You can make this conversion explicit by writing `timed _` or `timed(_,_)` instead of `timed`.\n",
       "       timed\n",
       "       ^\n",
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// To flush out error: missing argument list for method timed\n",
    "println(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BASE_LOCALDATE = 2017-01-01\n",
       "udf_months_between = UserDefinedFunction(<function1>,ShortType,Some(List(TimestampType)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "get_months_between: (to: java.sql.Timestamp)Short\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,ShortType,Some(List(TimestampType)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val BASE_TIMESTAMP = java.sql.Timestamp.valueOf(\"2017-01-01 00:00:00.0\")\n",
    "val BASE_LOCALDATE = LocalDate.parse(\"2017-01-01\").withDayOfMonth(1)\n",
    "\n",
    "def get_months_between(to: Timestamp): Short = {\n",
    "    val monthsBetween = ChronoUnit.MONTHS.between(\n",
    "        BASE_LOCALDATE,\n",
    "        to.toLocalDateTime().toLocalDate().withDayOfMonth(1)\n",
    "    )\n",
    "    monthsBetween.toShort\n",
    "}\n",
    "val udf_months_between = udf((t:Timestamp) => get_months_between(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passengerId: integer (nullable = true)\n",
      " |-- flightId: integer (nullable = true)\n",
      " |-- from: string (nullable = true)\n",
      " |-- to: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- direction: integer (nullable = false)\n",
      " |-- count: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightData = [passengerId: int, flightId: int ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 5 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transformations, no action yet\n",
    "val flightData = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"../resources/flightData.csv\")\n",
    "    .withColumn(\n",
    "        \"direction\", \n",
    "        when(lower(col(\"from\")) === \"uk\", 1)\n",
    "        .when(lower(col(\"to\"))   === \"uk\", -1)\n",
    "        .otherwise(0)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"count\", lit(1)\n",
    "    )\n",
    "    .orderBy(asc(\"passengerId\"), asc(\"date\"))\n",
    "\n",
    "flightData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----+---+-------------------+---------+-----+\n",
      "|passengerId|flightId|from| to|               date|direction|count|\n",
      "+-----------+--------+----+---+-------------------+---------+-----+\n",
      "|          1|       0|  cg| ir|2017-01-01 00:00:00|        0|    1|\n",
      "|          1|     901|  ir| at|2017-11-29 00:00:00|        0|    1|\n",
      "|          1|     940|  at| cn|2017-12-12 00:00:00|        0|    1|\n",
      "|          1|     972|  cn| ch|2017-12-22 00:00:00|        0|    1|\n",
      "|          1|     993|  ch| pk|2017-12-29 00:00:00|        0|    1|\n",
      "|          2|       0|  cg| ir|2017-01-01 00:00:00|        0|    1|\n",
      "|          3|       0|  cg| ir|2017-01-01 00:00:00|        0|    1|\n",
      "|          3|      32|  ir| sg|2017-01-10 00:00:00|        0|    1|\n",
      "|          3|     108|  sg| be|2017-02-06 00:00:00|        0|    1|\n",
      "|          3|     176|  be| ir|2017-03-05 00:00:00|        0|    1|\n",
      "|          4|       0|  cg| ir|2017-01-01 00:00:00|        0|    1|\n",
      "|          4|     200|  ir| no|2017-03-14 00:00:00|        0|    1|\n",
      "|          4|     561|  no| cn|2017-07-19 00:00:00|        0|    1|\n",
      "|          4|     602|  cn| sg|2017-08-03 00:00:00|        0|    1|\n",
      "|          4|     629|  sg| jo|2017-08-14 00:00:00|        0|    1|\n",
      "|          4|     772|  jo| ir|2017-10-08 00:00:00|        0|    1|\n",
      "|          4|     825|  ir| tj|2017-10-29 00:00:00|        0|    1|\n",
      "|          4|     991|  tj| at|2017-12-29 00:00:00|        0|    1|\n",
      "|          5|       0|  cg| ir|2017-01-01 00:00:00|        0|    1|\n",
      "|          6|       0|  cg| ir|2017-01-01 00:00:00|        0|    1|\n",
      "+-----------+--------+----+---+-------------------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flightData.createOrReplaceTempView(\"flightData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum over partition & range\n",
    "Number of flights per passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----+---+-------------------+---------+-----+-----------+\n",
      "|passengerId|flightId|from| to|               date|direction|count|num_flights|\n",
      "+-----------+--------+----+---+-------------------+---------+-----+-----------+\n",
      "|          1|       0|  cg| ir|2017-01-01 00:00:00|        0|    1|          4|\n",
      "|          1|     901|  ir| at|2017-11-29 00:00:00|        0|    1|          3|\n",
      "|          1|     940|  at| cn|2017-12-12 00:00:00|        0|    1|          2|\n",
      "|          1|     972|  cn| ch|2017-12-22 00:00:00|        0|    1|          1|\n",
      "|          1|     993|  ch| pk|2017-12-29 00:00:00|        0|    1|       null|\n",
      "|          2|       0|  cg| ir|2017-01-01 00:00:00|        0|    1|       null|\n",
      "|          3|       0|  cg| ir|2017-01-01 00:00:00|        0|    1|          3|\n",
      "|          3|      32|  ir| sg|2017-01-10 00:00:00|        0|    1|          2|\n",
      "|          3|     108|  sg| be|2017-02-06 00:00:00|        0|    1|          1|\n",
      "|          3|     176|  be| ir|2017-03-05 00:00:00|        0|    1|       null|\n",
      "|          4|       0|  cg| ir|2017-01-01 00:00:00|        0|    1|          7|\n",
      "|          4|     200|  ir| no|2017-03-14 00:00:00|        0|    1|          6|\n",
      "|          4|     561|  no| cn|2017-07-19 00:00:00|        0|    1|          5|\n",
      "|          4|     602|  cn| sg|2017-08-03 00:00:00|        0|    1|          4|\n",
      "|          4|     629|  sg| jo|2017-08-14 00:00:00|        0|    1|          3|\n",
      "|          4|     772|  jo| ir|2017-10-08 00:00:00|        0|    1|          2|\n",
      "|          4|     825|  ir| tj|2017-10-29 00:00:00|        0|    1|          1|\n",
      "|          4|     991|  tj| at|2017-12-29 00:00:00|        0|    1|       null|\n",
      "|          5|       0|  cg| ir|2017-01-01 00:00:00|        0|    1|       null|\n",
      "|          6|       0|  cg| ir|2017-01-01 00:00:00|        0|    1|          9|\n",
      "+-----------+--------+----+---+-------------------+---------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query = \n",
       "countSum = [passengerId: int, flightId: int ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n",
       "\"\n",
       "SELECT\n",
       "    f.*,\n",
       "    sum(count) OVER (\n",
       "        PARTITION BY passengerId\n",
       "        ORDER BY\n",
       "            passengerId ASC,\n",
       "            date DESC\n",
       "        ROWS BETWEEN\n",
       "            UNBOUNDED PRECEDING\n",
       "            AND\n",
       "            1 PRECEDING\n",
       "    ) as num_flights\n",
       "FROM\n",
       "    flightData f\n",
       "ORDER BY\n",
       "    passengerId, date\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 6 more fields]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var query = \"\"\"\n",
    "SELECT \n",
    "    f.*,\n",
    "    sum(count) OVER (\n",
    "        PARTITION BY passengerId \n",
    "        ORDER BY \n",
    "            passengerId ASC, \n",
    "            date DESC\n",
    "        ROWS BETWEEN \n",
    "            UNBOUNDED PRECEDING\n",
    "            AND \n",
    "            1 PRECEDING\n",
    "    ) as num_flights\n",
    "FROM\n",
    "    flightData f\n",
    "ORDER BY \n",
    "    passengerId, date\n",
    "\"\"\"\n",
    "\n",
    "val countSum = spark.sql(query)\n",
    "countSum.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag / Lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+------------------+\n",
      "|LAG_FROM_PREVIOUS_ROW|ROW_VALUE|LEAD_FROM_NEXT_ROW|\n",
      "+---------------------+---------+------------------+\n",
      "|                 null|        1|                 2|\n",
      "|                    1|        2|                 3|\n",
      "|                    2|        3|                 4|\n",
      "|                    3|        4|              null|\n",
      "+---------------------+---------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query = \n",
       "result = [LAG_FROM_PREVIOUS_ROW: int, ROW_VALUE: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "SELECT\n",
       "  lag(v) OVER (ORDER BY v) as LAG_FROM_PREVIOUS_ROW,\n",
       "  v as ROW_VALUE,\n",
       "  lead(v) OVER (ORDER BY v) as LEAD_FROM_NEXT_ROW\n",
       "FROM (\n",
       "  VALUES (1), (2), (3), (4)\n",
       ") t(v)\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[LAG_FROM_PREVIOUS_ROW: int, ROW_VALUE: int ... 1 more field]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var query = \"\"\"\n",
    "SELECT\n",
    "  lag(v) OVER (ORDER BY v) as LAG_FROM_PREVIOUS_ROW,\n",
    "  v as ROW_VALUE,\n",
    "  lead(v) OVER (ORDER BY v) as LEAD_FROM_NEXT_ROW\n",
    "FROM (\n",
    "  VALUES (1), (2), (3), (4)\n",
    ") t(v)\n",
    "\"\"\"\n",
    "var result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row Number in Window Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: Table or view not found: flightData; line 6 pos 4\n",
       "StackTrace:   at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:731)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:683)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:713)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:706)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n",
       "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:706)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:652)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n",
       "  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n",
       "  at scala.collection.immutable.List.foldLeft(List.scala:84)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:392)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n",
       "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n",
       "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n",
       "  ... 42 elided\n",
       "Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'flightdata' not found in database 'default';\n",
       "  at org.apache.spark.sql.catalyst.catalog.ExternalCatalog$class.requireTableExists(ExternalCatalog.scala:48)\n",
       "  at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireTableExists(InMemoryCatalog.scala:45)\n",
       "  at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.getTable(InMemoryCatalog.scala:326)\n",
       "  at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n",
       "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:728)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var query = \"\"\"\n",
    "SELECT \n",
    "    f.*,\n",
    "    ROW_NUMBER() OVER (PARTITION BY passengerId ORDER BY passengerId, date) as seq \n",
    "FROM\n",
    "    flightData f\n",
    "ORDER BY \n",
    "    passengerId, date\n",
    "\"\"\"\n",
    "\n",
    "val passageSequenced = spark.sql(query)\n",
    "passageSequenced\n",
    "    .filter($\"passengerId\" === 22)\n",
    "    .show()\n",
    "passageSequenced.createOrReplaceTempView(\"passageSequenced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAG / LEAD \n",
    "Flights starting and ending at UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+---+---------+---+------+---------+\n",
      "|passengerId|from| to|direction|seq|return|countries|\n",
      "+-----------+----+---+---------+---+------+---------+\n",
      "|         16|  cg| uk|       -1|  5|  null|     null|\n",
      "|         16|  uk| cg|        1|  6|  null|     null|\n",
      "|         22|  iq| uk|       -1| 10|  null|     null|\n",
      "|         22|  uk| nl|        1| 11|    15|        4|\n",
      "|         22|  at| uk|       -1| 15|  null|     null|\n",
      "|         22|  uk| bm|        1| 16|  null|     null|\n",
      "|         52|  se| uk|       -1|  6|  null|     null|\n",
      "|         52|  uk| cn|        1|  7|  null|     null|\n",
      "|         53|  ch| uk|       -1|  6|  null|     null|\n",
      "|         53|  uk| se|        1|  7|     8|        1|\n",
      "|         53|  se| uk|       -1|  8|  null|     null|\n",
      "|         53|  uk| tj|        1|  9|    13|        4|\n",
      "|         53|  th| uk|       -1| 13|  null|     null|\n",
      "|         72|  tj| uk|       -1|  9|  null|     null|\n",
      "|         72|  uk| iq|        1| 10|  null|     null|\n",
      "|         82|  iq| uk|       -1|  4|  null|     null|\n",
      "|         82|  uk| se|        1|  5|  null|     null|\n",
      "|        108|  cg| uk|       -1|  5|  null|     null|\n",
      "|        108|  uk| cn|        1|  6|  null|     null|\n",
      "|        127|  il| uk|       -1|  8|  null|     null|\n",
      "+-----------+----+---+---------+---+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "closedPassageQuery = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n",
       "\"\n",
       "SELECT\n",
       "    passengerId,\n",
       "    from, to,\n",
       "    direction,\n",
       "    seq,\n",
       "    --------------------------------------------------------------------------------\n",
       "    -- For a departure flight, take the the return flight, if there is, seq num\n",
       "    --------------------------------------------------------------------------------\n",
       "    CASE\n",
       "        WHEN direction == 1\n",
       "        THEN lead(seq) OVER (PARTITION BY passengerId ORDER BY seq)\n",
       "    END AS return,\n",
       "    --------------------------------------------------------------------------------\n",
       "    -- For a departure flight, count the visiting countries, if returned.\n",
       "    --------------------------------------------------------------------------------\n",
       "    CASE\n",
       "        WHEN direction == 1\n",
       "        THEN lead(seq) OVER (PARTITION BY passeng...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val closedPassageQuery = \"\"\"\n",
    "SELECT \n",
    "    passengerId, \n",
    "    from, to, \n",
    "    direction, \n",
    "    seq,\n",
    "    -------------------------------------------------------------------------------- \n",
    "    -- For a departure flight, take the the return flight, if there is, seq num\n",
    "    -------------------------------------------------------------------------------- \n",
    "    CASE \n",
    "        WHEN direction == 1\n",
    "        THEN lead(seq) OVER (PARTITION BY passengerId ORDER BY seq)\n",
    "    END AS return,\n",
    "    -------------------------------------------------------------------------------- \n",
    "    -- For a departure flight, count the visiting countries, if returned.\n",
    "    -------------------------------------------------------------------------------- \n",
    "    CASE \n",
    "        WHEN direction == 1\n",
    "        THEN lead(seq) OVER (PARTITION BY passengerId ORDER BY seq) - seq\n",
    "    END AS countries\n",
    "FROM passageSequenced p\n",
    "WHERE \n",
    "    direction != 0\n",
    "    AND EXISTS (  \n",
    "        SELECT passengerId\n",
    "        FROM\n",
    "            passageSequenced\n",
    "        WHERE \n",
    "            direction != 0 AND\n",
    "            passengerId == p.passengerId\n",
    "        GROUP BY\n",
    "            passengerId\n",
    "        Having count(DISTINCT direction) == 2\n",
    "    )\n",
    "ORDER BY \n",
    "    passengerId, seq\n",
    "\"\"\"\n",
    "\n",
    "val closedPassages = spark.sql(closedPassageQuery)\n",
    "closedPassages.show()\n",
    "closedPassages.createOrReplaceTempView(\"closedPassages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|passengerId|countries|\n",
      "+-----------+---------+\n",
      "|         22|        4|\n",
      "|         53|        4|\n",
      "|        167|        2|\n",
      "|        204|        3|\n",
      "|        227|        1|\n",
      "|        258|        3|\n",
      "|        281|        9|\n",
      "|        305|        3|\n",
      "|        309|        9|\n",
      "|        313|        8|\n",
      "|        315|        3|\n",
      "|        334|        9|\n",
      "|        340|        5|\n",
      "|        348|        3|\n",
      "|        386|        1|\n",
      "|        478|        1|\n",
      "|        494|        9|\n",
      "|        529|        2|\n",
      "|        615|        4|\n",
      "|        652|        1|\n",
      "+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "queryVisitedCountries = \n",
       "visitedCountries = [passengerId: int, countries: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "SELECT\n",
       "    passengerId,\n",
       "    max(countries) as countries\n",
       "FROM closedPassages\n",
       "WHERE\n",
       "    countries IS NOT NULL\n",
       "GROUP BY\n",
       "    passengerId\n",
       "ORDER BY\n",
       "    passengerId\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, countries: int]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var queryVisitedCountries = \"\"\"\n",
    "SELECT \n",
    "    passengerId,\n",
    "    max(countries) as countries\n",
    "FROM closedPassages\n",
    "WHERE \n",
    "    countries IS NOT NULL\n",
    "GROUP BY \n",
    "    passengerId\n",
    "ORDER BY \n",
    "    passengerId\n",
    "\"\"\"\n",
    "\n",
    "val visitedCountries = spark.sql(queryVisitedCountries)\n",
    "visitedCountries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "1. Data is clearned and not errorneous\n",
    "2. Timezone consideration is not required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.time.temporal.ChronoUnit\n",
    "import java.time.{Period, LocalDate, Instant}\n",
    "import java.sql.Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parition control based on core availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NUM_CORES = 4\n",
       "NUM_PARTITIONS = 3\n",
       "spark = <lazy>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<lazy>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NUM_CORES = 4\n",
    "val NUM_PARTITIONS = 3\n",
    "\n",
    "lazy val spark: SparkSession = SparkSession.builder()\n",
    "    .master(\"local\")\n",
    "    .appName(\"flight\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FLIGHTDATA_CSV_PATH = ../resources/flightData.csv\n",
       "PASSENGER_CSV_PATH = ../resources/passengers.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "../resources/passengers.csv"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val FLIGHTDATA_CSV_PATH = \"../resources/flightData.csv\"\n",
    "val PASSENGER_CSV_PATH = \"../resources/passengers.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timing = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "timed: [T](label: String, code: => T)T\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val timing = new StringBuffer\n",
    "def timed[T](label: String, code: => T): T = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val result = code\n",
    "    val stop = System.currentTimeMillis()\n",
    "    timing.append(s\"Processing $label took ${stop - start} ms.\\n\")\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:46: error: missing argument list for method timed\n",
       "Unapplied methods are only converted to functions when a function type is expected.\n",
       "You can make this conversion explicit by writing `timed _` or `timed(_,_)` instead of `timed`.\n",
       "       timed\n",
       "       ^\n",
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// To flush out error: missing argument list for method timed\n",
    "println(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BASE_LOCALDATE = 2017-01-01\n",
       "udf_months_between = UserDefinedFunction(<function1>,ShortType,Some(List(TimestampType)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "get_months_between: (to: java.sql.Timestamp)Short\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,ShortType,Some(List(TimestampType)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val BASE_TIMESTAMP = java.sql.Timestamp.valueOf(\"2017-01-01 00:00:00.0\")\n",
    "val BASE_LOCALDATE = LocalDate.parse(\"2017-01-01\").withDayOfMonth(1)\n",
    "\n",
    "def get_months_between(to: Timestamp): Short = {\n",
    "    val monthsBetween = ChronoUnit.MONTHS.between(\n",
    "        BASE_LOCALDATE,\n",
    "        to.toLocalDateTime().toLocalDate().withDayOfMonth(1)\n",
    "    )\n",
    "    monthsBetween.toShort\n",
    "}\n",
    "val udf_months_between = udf((t:Timestamp) => get_months_between(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total flights per month\n",
    "Month is in-between the first day of the month and the first day of the next month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n",
      "+--------------------+-----+\n",
      "|               month|count|\n",
      "+--------------------+-----+\n",
      "|[2016-12-29 10:00...|   97|\n",
      "|[2017-01-26 10:00...|   73|\n",
      "|[2017-02-23 10:00...|   82|\n",
      "|[2017-03-23 10:00...|   92|\n",
      "|[2017-04-20 10:00...|   92|\n",
      "|[2017-05-18 10:00...|   71|\n",
      "|[2017-06-15 10:00...|   87|\n",
      "|[2017-07-13 10:00...|   76|\n",
      "|[2017-08-10 10:00...|   85|\n",
      "|[2017-09-07 10:00...|   76|\n",
      "|[2017-10-05 10:00...|   75|\n",
      "|[2017-11-30 10:00...|   94|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightsPerMonth = [month: struct<start: timestamp, end: timestamp>, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[month: struct<start: timestamp, end: timestamp>, count: bigint]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transformations, no action yet\n",
    "val flightsPerMonth = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"../resources/flightData.csv\")\n",
    "    .select(\"flightId\", \"date\")\n",
    "    .distinct()\n",
    "    .groupBy(\n",
    "        window(trunc(col(\"date\"), \"month\"), \"4 weeks\")\n",
    "    )\n",
    "    .count\n",
    "    .orderBy(asc(\"window\"))\n",
    "    .withColumnRenamed(\"window\", \"month\")\n",
    "\n",
    "flightsPerMonth.printSchema()\n",
    "flightsPerMonth.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Transformations, no action yet\n",
    "val flightsPerMonth = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(FLIGHTDATA_CSV_PATH)\n",
    "    .select(\"flightId\", \"date\")\n",
    "    .distinct()\n",
    "    .groupBy(\n",
    "        trunc(col(\"date\"), \"month\").alias(\"Month\")\n",
    "    )\n",
    "    .agg(count(\"flightId\").alias(\"Number of Flights\"))\n",
    "    .orderBy(asc(\"Month\"))\n",
    "    .withColumn(\n",
    "        \"Month\", udf_months_between(col(\"Month\"))\n",
    "    )\n",
    "\n",
    "flightsPerMonth.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               month|count|\n",
      "+--------------------+-----+\n",
      "|[2016-12-31 10:00...|    5|\n",
      "|[2017-01-01 10:00...|    6|\n",
      "|[2017-01-02 10:00...|    5|\n",
      "|[2017-01-03 10:00...|    3|\n",
      "|[2017-01-04 10:00...|    1|\n",
      "|[2017-01-06 10:00...|    2|\n",
      "|[2017-01-07 10:00...|    4|\n",
      "|[2017-01-08 10:00...|    6|\n",
      "|[2017-01-09 10:00...|    9|\n",
      "|[2017-01-10 10:00...|    4|\n",
      "|[2017-01-11 10:00...|    1|\n",
      "|[2017-01-12 10:00...|    5|\n",
      "|[2017-01-13 10:00...|    1|\n",
      "|[2017-01-14 10:00...|    3|\n",
      "|[2017-01-15 10:00...|    2|\n",
      "|[2017-01-16 10:00...|    3|\n",
      "|[2017-01-17 10:00...|    1|\n",
      "|[2017-01-18 10:00...|    3|\n",
      "|[2017-01-19 10:00...|    6|\n",
      "|[2017-01-20 10:00...|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsPerMonth.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent flyers\n",
    "Top 100 frequent flyers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top frequent flyers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passengerId: integer (nullable = true)\n",
      " |-- numberOfFlights: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TOP_N = 100\n",
       "frequentFlyers = [passengerId: int, numberOfFlights: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, numberOfFlights: bigint]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val TOP_N = 100\n",
    "val frequentFlyers = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"../resources/flightData.csv\")\n",
    "    .select(\"passengerId\")\n",
    "    .groupBy(\"passengerId\")\n",
    "    .count\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .withColumnRenamed(\"count\", \"numberOfFlights\")\n",
    "    //--------------------------------------------------------------------------------\n",
    "    // TOP_N flyers \n",
    "    //--------------------------------------------------------------------------------\n",
    "    .limit(LIMIT)\n",
    "    //--------------------------------------------------------------------------------\n",
    "    // Re-sort for passengerId match\n",
    "    //--------------------------------------------------------------------------------\n",
    "    .orderBy(asc(\"passengerId\"))\n",
    "    .persist\n",
    "\n",
    "frequentFlyers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passengerId: integer (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LIMIT = 100\n",
       "passengers = [passengerId: int, firstName: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, firstName: string ... 1 more field]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val LIMIT = 100\n",
    "val passengers = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(PASSENGER_CSV_PATH)\n",
    "    //--------------------------------------------------------------------------------\n",
    "    // Sort for passengerId match\n",
    "    //--------------------------------------------------------------------------------\n",
    "    .orderBy(asc(\"passengerId\"))\n",
    "    .persist\n",
    "\n",
    "passengers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent flyer listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequentFlyers.createOrReplaceTempView(\"frequentFlyers\")\n",
    "passengers.createOrReplaceTempView(\"passengers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "queryTopFrequetFlyers = \n",
       "topFrequentFlyers = [passenger_id: int, number_of_flights: bigint ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "SELECT\n",
       "    f.passengerId AS passenger_id,\n",
       "    f.numberOfFlights AS number_of_flights,\n",
       "    p.firstName AS first_name,\n",
       "    p.lastName as last_name\n",
       "FROM\n",
       "    frequentFlyers f\n",
       "    INNER JOIN passengers p\n",
       "    ON f.passengerId = p.passengerId\n",
       "ORDER BY number_of_flights DESC\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passenger_id: int, number_of_flights: bigint ... 2 more fields]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val queryTopFrequetFlyers = \"\"\"\n",
    "SELECT \n",
    "    f.passengerId AS passenger_id,\n",
    "    f.numberOfFlights AS number_of_flights,\n",
    "    p.firstName AS first_name,\n",
    "    p.lastName as last_name\n",
    "FROM\n",
    "    frequentFlyers f\n",
    "    INNER JOIN passengers p\n",
    "    ON f.passengerId = p.passengerId\n",
    "ORDER BY number_of_flights DESC\n",
    "\"\"\"\n",
    "\n",
    "val topFrequentFlyers = spark.sql(queryTopFrequetFlyers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+----------+---------+\n",
      "|passenger_id|number_of_flights|first_name|last_name|\n",
      "+------------+-----------------+----------+---------+\n",
      "|        2068|               32|   Yolande|     Pete|\n",
      "|        1677|               27| Katherina| Vasiliki|\n",
      "|        4827|               27|     Jaime|    Renay|\n",
      "|        3173|               26|  Sunshine|    Scott|\n",
      "|        8961|               26|     Ginny|    Clara|\n",
      "+------------+-----------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topFrequentFlyers\n",
    "    // Coalesce to save in the driver node as one file, otherwise no need\n",
    "    .coalesce(1)   \n",
    "    // .persist\n",
    "    .write\n",
    "    .format(\"csv\")\n",
    "    .mode(SaveMode.Overwrite)\n",
    "    .option(\"header\", \"true\")\n",
    "    .save(\"topFrequentFlyers\") \n",
    "\n",
    "topFrequentFlyers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "frequentFlyers.unpersist\n",
    "passengers.unpersist\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longest passages\n",
    "Find the greatest number of countries a passenger has been in without being in the UK. For example, if the countries a passenger was in were: UK -> FR -> US -> CN -> UK -> DE -> UK, the correct answer would be 3 countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Passage\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import java.sql.Timestamp\n",
    "case class Passage (\n",
    "    passengerId: Int,\n",
    "    flightId: String,\n",
    "    from: String,\n",
    "    to: String,\n",
    "    date: Timestamp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passengerId: integer (nullable = true)\n",
      " |-- from: string (nullable = true)\n",
      " |-- to: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightData = [passengerId: int, from: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, from: string ... 1 more field]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transformations, no action yet\n",
    "val flightData = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(FLIGHTDATA_CSV_PATH)\n",
    "    .orderBy(asc(\"passengerId\"), asc(\"date\"))\n",
    "    .select(\"passengerId\", \"from\", \"to\")\n",
    "\n",
    "flightData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+---+\n",
      "|passengerId|from| to|\n",
      "+-----------+----+---+\n",
      "|          1|  cg| ir|\n",
      "|          1|  ir| at|\n",
      "|          1|  at| cn|\n",
      "|          1|  cn| ch|\n",
      "|          1|  ch| pk|\n",
      "|          2|  cg| ir|\n",
      "|          3|  cg| ir|\n",
      "|          3|  ir| sg|\n",
      "|          3|  sg| be|\n",
      "|          3|  be| ir|\n",
      "|          4|  cg| ir|\n",
      "|          4|  ir| no|\n",
      "|          4|  no| cn|\n",
      "|          4|  cn| sg|\n",
      "|          4|  sg| jo|\n",
      "|          4|  jo| ir|\n",
      "|          4|  ir| tj|\n",
      "|          4|  tj| at|\n",
      "|          5|  cg| ir|\n",
      "|          6|  cg| ir|\n",
      "+-----------+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightDataRDD = MapPartitionsRDD[214] at rdd at <console>:59\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[214] at rdd at <console>:59"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Row\n",
    "val flightDataRDD: RDD[Row] = flightData.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,cg,ir]\n",
      "[1,ir,at]\n",
      "[1,at,cn]\n",
      "[1,cn,ch]\n",
      "[1,ch,pk]\n"
     ]
    }
   ],
   "source": [
    "flightDataRDD.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12516,CompactBuffer((no,be), (be,au), (au,cn), (cn,au), (au,sg), (sg,tk), (tk,se), (se,us), (us,au), (au,be)))\n",
      "(12456,CompactBuffer((dk,dk)))\n",
      "(14508,CompactBuffer((ca,co)))\n",
      "(3456,CompactBuffer((au,ch), (ch,fr), (fr,pk)))\n",
      "(7608,CompactBuffer((sg,ar), (ar,cl), (cl,sg), (sg,iq), (iq,jo), (jo,no), (no,il), (il,uk), (uk,ca), (ca,jo), (jo,ch), (ch,il), (il,jo), (jo,us), (us,au)))\n"
     ]
    }
   ],
   "source": [
    "passages.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:36: error: not found: value when\n",
       "               when(lower(col(\"from\")) === \"uk\", 1)\n",
       "               ^\n",
       "<console>:36: error: not found: value lower\n",
       "               when(lower(col(\"from\")) === \"uk\", 1)\n",
       "                    ^\n",
       "<console>:36: error: not found: value col\n",
       "               when(lower(col(\"from\")) === \"uk\", 1)\n",
       "                          ^\n",
       "<console>:37: error: not found: value lower\n",
       "               .when(lower(col(\"to\"))   === \"uk\", -1)\n",
       "                     ^\n",
       "<console>:37: error: not found: value col\n",
       "               .when(lower(col(\"to\"))   === \"uk\", -1)\n",
       "                           ^\n",
       "<console>:43: error: not found: value asc\n",
       "           .orderBy(asc(\"passengerId\"), asc(\"date\"))\n",
       "                    ^\n",
       "<console>:43: error: not found: value asc\n",
       "           .orderBy(asc(\"passengerId\"), asc(\"date\"))\n",
       "                                        ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transformations, no action yet\n",
    "val passageDirections = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"../resources/flightData.csv\")\n",
    "    .withColumn(\n",
    "        \"direction\", \n",
    "        when(lower(col(\"from\")) === \"uk\", 1)\n",
    "        .when(lower(col(\"to\"))   === \"uk\", -1)\n",
    "        .otherwise(0)\n",
    "    )\n",
    "    .orderBy(asc(\"passengerId\"), asc(\"date\"))\n",
    "\n",
    "passageDirections.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----+---+-------------------+---------+\n",
      "|passengerId|flightId|from| to|               date|direction|\n",
      "+-----------+--------+----+---+-------------------+---------+\n",
      "|         14|       0|  cg| ir|2017-01-01 00:00:00|        0|\n",
      "|         14|     200|  ir| no|2017-03-14 00:00:00|        0|\n",
      "|         14|     222|  no| ir|2017-03-19 00:00:00|        0|\n",
      "|         22|       0|  cg| ir|2017-01-01 00:00:00|        0|\n",
      "|         22|      32|  ir| sg|2017-01-10 00:00:00|        0|\n",
      "|         22|     147|  sg| tj|2017-02-22 00:00:00|        0|\n",
      "|         22|     162|  tj| iq|2017-02-27 00:00:00|        0|\n",
      "|         22|     279|  iq| il|2017-04-11 00:00:00|        0|\n",
      "|         22|     290|  il| pk|2017-04-15 00:00:00|        0|\n",
      "|         22|     330|  pk| co|2017-04-27 00:00:00|        0|\n",
      "|         22|     335|  co| at|2017-04-29 00:00:00|        0|\n",
      "|         22|     363|  at| iq|2017-05-09 00:00:00|        0|\n",
      "|         22|     681|  iq| uk|2017-09-06 00:00:00|       -1|\n",
      "|         22|     786|  uk| nl|2017-10-15 00:00:00|        1|\n",
      "|         22|     789|  nl| th|2017-10-16 00:00:00|        0|\n",
      "|         22|     811|  th| cg|2017-10-23 00:00:00|        0|\n",
      "|         22|     842|  cg| at|2017-11-08 00:00:00|        0|\n",
      "|         22|     856|  at| uk|2017-11-15 00:00:00|       -1|\n",
      "|         22|     881|  uk| bm|2017-11-22 00:00:00|        1|\n",
      "+-----------+--------+----+---+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//passageDirections.show(300)\n",
    "//passageDirections.filter(col(\"passengerId\") isin (16, 22, 47, 52)).show(50)\n",
    "passageDirections.filter(col(\"passengerId\") isin (22,14)).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:141: error: value toDs is not a member of org.apache.spark.sql.DataFrame\n",
       "       hoge.toDs()\n",
       "            ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hoge = passageDirections\n",
    "    .select(\"passengerId\", \"from\", \"to\")\n",
    "\n",
    "hoge.toDs()\n",
    "hoge.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "passageDirections.createOrReplaceTempView(\"passageDirections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----+---+-------------------+---------+------+--------+\n",
      "|passengerId|flightId|from| to|               date|direction|seqnum|seqnum_2|\n",
      "+-----------+--------+----+---+-------------------+---------+------+--------+\n",
      "|         22|       0|  cg| ir|2017-01-01 00:00:00|        0|     1|       1|\n",
      "|         22|      32|  ir| sg|2017-01-10 00:00:00|        0|     2|       2|\n",
      "|         22|     147|  sg| tj|2017-02-22 00:00:00|        0|     3|       3|\n",
      "|         22|     162|  tj| iq|2017-02-27 00:00:00|        0|     4|       4|\n",
      "|         22|     279|  iq| il|2017-04-11 00:00:00|        0|     5|       5|\n",
      "|         22|     290|  il| pk|2017-04-15 00:00:00|        0|     6|       6|\n",
      "|         22|     330|  pk| co|2017-04-27 00:00:00|        0|     7|       7|\n",
      "|         22|     335|  co| at|2017-04-29 00:00:00|        0|     8|       8|\n",
      "|         22|     363|  at| iq|2017-05-09 00:00:00|        0|     9|       9|\n",
      "|         22|     681|  iq| uk|2017-09-06 00:00:00|       -1|    10|       1|\n",
      "|         22|     786|  uk| nl|2017-10-15 00:00:00|        1|    11|       1|\n",
      "|         22|     789|  nl| th|2017-10-16 00:00:00|        0|    12|      10|\n",
      "|         22|     811|  th| cg|2017-10-23 00:00:00|        0|    13|      11|\n",
      "|         22|     842|  cg| at|2017-11-08 00:00:00|        0|    14|      12|\n",
      "|         22|     856|  at| uk|2017-11-15 00:00:00|       -1|    15|       2|\n",
      "|         22|     881|  uk| bm|2017-11-22 00:00:00|        1|    16|       2|\n",
      "+-----------+--------+----+---+-------------------+---------+------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query = \n",
       "result = [passengerId: int, flightId: int ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "SELECT\n",
       "    passageDirections.*,\n",
       "    row_number() OVER (PARTITION BY passengerId ORDER BY date)            AS seqnum,\n",
       "    row_number() OVER (PARTITION BY passengerId, direction ORDER BY date) AS seqnum_2\n",
       "FROM passageDirections\n",
       "WHERE passengerId in (22)\n",
       "ORDER BY passengerId, date\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 6 more fields]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var query = \"\"\"\n",
    "SELECT \n",
    "    passageDirections.*,\n",
    "    row_number() OVER (PARTITION BY passengerId ORDER BY date)            AS seqnum,\n",
    "    row_number() OVER (PARTITION BY passengerId, direction ORDER BY date) AS seqnum_2\n",
    "FROM passageDirections\n",
    "WHERE passengerId in (22)\n",
    "ORDER BY passengerId, date\n",
    "\"\"\"\n",
    "\n",
    "var result = spark.sql(query)\n",
    "\n",
    "result.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "queryPassages = \n",
       "passages = [passengerId: int, min(date): timestamp ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "SELECT\n",
       "    passengerId,\n",
       "    min(date),\n",
       "    max(date),\n",
       "    count(*) AS num_passages\n",
       "FROM (\n",
       "    SELECT\n",
       "        passageDirections.*,\n",
       "        row_number() OVER (PARTITION BY passengerId ORDER BY date)            AS seqnum,\n",
       "        row_number() OVER (PARTITION BY passengerId, direction ORDER BY date) AS seqnum_2\n",
       "    FROM passageDirections\n",
       "    )\n",
       "WHERE direction = 0\n",
       "GROUP BY passengerId, (seqnum - seqnum_2), direction\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, min(date): timestamp ... 2 more fields]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val queryPassages = \"\"\"\n",
    "SELECT \n",
    "    passengerId, \n",
    "    min(date), \n",
    "    max(date), \n",
    "    count(*) AS num_passages\n",
    "FROM (\n",
    "    SELECT \n",
    "        passageDirections.*,\n",
    "        row_number() OVER (PARTITION BY passengerId ORDER BY date)            AS seqnum,\n",
    "        row_number() OVER (PARTITION BY passengerId, direction ORDER BY date) AS seqnum_2\n",
    "    FROM passageDirections\n",
    "    )\n",
    "WHERE direction = 0\n",
    "GROUP BY passengerId, (seqnum - seqnum_2), direction\n",
    "\"\"\"\n",
    "\n",
    "val passages = spark.sql(queryPassages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+------------+\n",
      "|passengerId|          min(date)|          max(date)|num_passages|\n",
      "+-----------+-------------------+-------------------+------------+\n",
      "|         14|2017-01-01 00:00:00|2017-03-19 00:00:00|           3|\n",
      "|         18|2017-01-01 00:00:00|2017-12-20 00:00:00|          14|\n",
      "|         25|2017-01-01 00:00:00|2017-12-16 00:00:00|           2|\n",
      "|         38|2017-01-01 00:00:00|2017-12-22 00:00:00|           4|\n",
      "|         46|2017-01-01 00:00:00|2017-12-25 00:00:00|           6|\n",
      "|         50|2017-01-01 00:00:00|2017-12-12 00:00:00|           6|\n",
      "|         73|2017-01-01 00:00:00|2017-07-13 00:00:00|           5|\n",
      "|         97|2017-01-01 00:00:00|2017-12-16 00:00:00|           8|\n",
      "|        161|2017-01-01 00:00:00|2017-01-01 00:00:00|           1|\n",
      "|        172|2017-01-01 00:00:00|2017-04-27 00:00:00|           9|\n",
      "|        186|2017-01-01 00:00:00|2017-07-15 00:00:00|           9|\n",
      "|        225|2017-01-01 00:00:00|2017-12-22 00:00:00|           9|\n",
      "|        232|2017-01-01 00:00:00|2017-09-21 00:00:00|          11|\n",
      "|        233|2017-01-01 00:00:00|2017-12-29 00:00:00|          10|\n",
      "|        248|2017-01-01 00:00:00|2017-12-27 00:00:00|          15|\n",
      "|        254|2017-01-01 00:00:00|2017-11-16 00:00:00|          13|\n",
      "|        257|2017-01-01 00:00:00|2017-08-27 00:00:00|           7|\n",
      "|        263|2017-01-01 00:00:00|2017-12-19 00:00:00|           7|\n",
      "|        280|2017-01-01 00:00:00|2017-06-28 00:00:00|           8|\n",
      "|        282|2017-01-01 00:00:00|2017-11-08 00:00:00|           8|\n",
      "+-----------+-------------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "passages.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+\n",
      "|   A|  B|   C|\n",
      "+----+---+----+\n",
      "|null|  1|   2|\n",
      "|   1|  2|   3|\n",
      "|   2|  3|   4|\n",
      "|   3|  4|null|\n",
      "+----+---+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query = \n",
       "result = [A: int, B: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "SELECT\n",
       "  lag(v) OVER (ORDER BY v) as A,\n",
       "  v as B,\n",
       "  lead(v) OVER (ORDER BY v) as C\n",
       "FROM (\n",
       "  VALUES (1), (2), (3), (4)\n",
       ") t(v)\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[A: int, B: int ... 1 more field]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var query = \"\"\"\n",
    "SELECT\n",
    "  lag(v) OVER (ORDER BY v) as A,\n",
    "  v as B,\n",
    "  lead(v) OVER (ORDER BY v) as C\n",
    "FROM (\n",
    "  VALUES (1), (2), (3), (4)\n",
    ") t(v)\n",
    "\"\"\"\n",
    "var result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

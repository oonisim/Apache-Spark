{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "1. Data is clearned and not errorneous\n",
    "2. Timezone consideration is not required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.time.temporal.ChronoUnit\n",
    "import java.time.{Period, LocalDate, Instant}\n",
    "import java.sql.Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parition control based on core availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NUM_CORES = 4\n",
       "NUM_PARTITIONS = 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NUM_CORES = 4\n",
    "val NUM_PARTITIONS = 3\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timing = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "timed: [T](label: String, code: => T)T\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val timing = new StringBuffer\n",
    "def timed[T](label: String, code: => T): T = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val result = code\n",
    "    val stop = System.currentTimeMillis()\n",
    "    timing.append(s\"Processing $label took ${stop - start} ms.\\n\")\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:93: error: missing argument list for method timed\n",
       "Unapplied methods are only converted to functions when a function type is expected.\n",
       "You can make this conversion explicit by writing `timed _` or `timed(_,_)` instead of `timed`.\n",
       "       timed\n",
       "       ^\n",
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// To flush out error: missing argument list for method timed\n",
    "println(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BASE_DATE = 2017-01-01\n",
       "BASE_LOCALDATE = 2017-01-01\n",
       "udf_months_between = UserDefinedFunction(<function1>,ShortType,Some(List(TimestampType)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "get_months_between: (to: java.sql.Timestamp)Short\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,ShortType,Some(List(TimestampType)))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val BASE_TIMESTAMP = java.sql.Timestamp.valueOf(\"2017-01-01 00:00:00.0\")\n",
    "val BASE_LOCALDATE = LocalDate.parse(\"2017-01-01\").withDayOfMonth(1)\n",
    "\n",
    "def get_months_between(to: Timestamp): Short = {\n",
    "    val monthsBetween = ChronoUnit.MONTHS.between(\n",
    "        BASE_LOCALDATE,\n",
    "        to.toLocalDateTime().toLocalDate().withDayOfMonth(1)\n",
    "    )\n",
    "    monthsBetween.toShort\n",
    "}\n",
    "val udf_months_between = udf((t:Timestamp) => get_months_between(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total flights per month\n",
    "Month is in-between the first day of the month and the first day of the next month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: short (nullable = false)\n",
      " |-- Flights: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightsPerMonth = [month: smallint, Flights: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[month: smallint, Flights: bigint]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transformations, no action yet\n",
    "val flightsPerMonth = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"../resources/flightData.csv\")\n",
    "    .withColumn(\n",
    "        \"month\", udf_months_between(col(\"date\"))\n",
    "    )\n",
    "    .select(\"flightId\", \"month\")\n",
    "    .distinct()\n",
    "    .groupBy(\"month\")\n",
    "    .agg(count(\"flightId\"))\n",
    "    .orderBy(asc(\"month\"))\n",
    "    .withColumnRenamed(\"count(flightId)\", \"Flights\")\n",
    "\n",
    "flightsPerMonth.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Action\n",
    "flightsPerMonth\n",
    "    // Coalesce to save in the driver node as one file, otherwise no need\n",
    "    .coalesce(1)   \n",
    "    // .persist\n",
    "    .write\n",
    "    .format(\"csv\")\n",
    "    .mode(SaveMode.Overwrite)\n",
    "    .option(\"header\", \"true\")\n",
    "    .save(\"flightsPerMonth\") \n",
    "\n",
    "/*\n",
    "flights.show(5)\n",
    "flights.unpersist()\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent flyers\n",
    "Top 100 frequent flyers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top frequent flyers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passengerId: integer (nullable = true)\n",
      " |-- numberOfFlights: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TOP_N = 100\n",
       "frequentFlyers = [passengerId: int, numberOfFlights: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, numberOfFlights: bigint]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val TOP_N = 100\n",
    "val frequentFlyers = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"../resources/flightData.csv\")\n",
    "    .select(\"passengerId\")\n",
    "    .groupBy(\"passengerId\")\n",
    "    .count\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .withColumnRenamed(\"count\", \"numberOfFlights\")\n",
    "    //--------------------------------------------------------------------------------\n",
    "    // TOP_N flyers \n",
    "    //--------------------------------------------------------------------------------\n",
    "    .limit(LIMIT)\n",
    "    //--------------------------------------------------------------------------------\n",
    "    // Re-sort for passengerId match\n",
    "    //--------------------------------------------------------------------------------\n",
    "    .orderBy(asc(\"passengerId\"))\n",
    "    .persist\n",
    "\n",
    "frequentFlyers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passengerId: integer (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LIMIT = 100\n",
       "passengers = [passengerId: int, firstName: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, firstName: string ... 1 more field]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val LIMIT = 100\n",
    "val passengers = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"../resources/passengers.csv\")\n",
    "    //--------------------------------------------------------------------------------\n",
    "    // Sort for passengerId match\n",
    "    //--------------------------------------------------------------------------------\n",
    "    .orderBy(asc(\"passengerId\"))\n",
    "    .persist\n",
    "\n",
    "passengers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent flyer listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequentFlyers.createOrReplaceTempView(\"frequentFlyers\")\n",
    "passengers.createOrReplaceTempView(\"passengers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "queryTopFrequetFlyers = \n",
       "topFrequentFlyers = [passenger_id: int, number_of_flights: bigint ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "SELECT\n",
       "    f.passengerId AS passenger_id,\n",
       "    f.numberOfFlights AS number_of_flights,\n",
       "    p.firstName AS first_name,\n",
       "    p.lastName as last_name\n",
       "FROM\n",
       "    frequentFlyers f\n",
       "    INNER JOIN passengers p\n",
       "    ON f.passengerId = p.passengerId\n",
       "ORDER BY number_of_flights DESC\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passenger_id: int, number_of_flights: bigint ... 2 more fields]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val queryTopFrequetFlyers = \"\"\"\n",
    "SELECT \n",
    "    f.passengerId AS passenger_id,\n",
    "    f.numberOfFlights AS number_of_flights,\n",
    "    p.firstName AS first_name,\n",
    "    p.lastName as last_name\n",
    "FROM\n",
    "    frequentFlyers f\n",
    "    INNER JOIN passengers p\n",
    "    ON f.passengerId = p.passengerId\n",
    "ORDER BY number_of_flights DESC\n",
    "\"\"\"\n",
    "\n",
    "val topFrequentFlyers = spark.sql(queryTopFrequetFlyers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+----------+---------+\n",
      "|passenger_id|number_of_flights|first_name|last_name|\n",
      "+------------+-----------------+----------+---------+\n",
      "|        2068|               32|   Yolande|     Pete|\n",
      "|        1677|               27| Katherina| Vasiliki|\n",
      "|        4827|               27|     Jaime|    Renay|\n",
      "|        3173|               26|  Sunshine|    Scott|\n",
      "|        8961|               26|     Ginny|    Clara|\n",
      "+------------+-----------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topFrequentFlyers\n",
    "    // Coalesce to save in the driver node as one file, otherwise no need\n",
    "    .coalesce(1)   \n",
    "    // .persist\n",
    "    .write\n",
    "    .format(\"csv\")\n",
    "    .mode(SaveMode.Overwrite)\n",
    "    .option(\"header\", \"true\")\n",
    "    .save(\"topFrequentFlyers\") \n",
    "\n",
    "topFrequentFlyers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[passengerId: int, firstName: string ... 1 more field]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequentFlyers.unpersist\n",
    "passengers.unpersist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

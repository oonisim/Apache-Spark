{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 23 - RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Error parsing magics!\n",
       "Message: Magics [run] do not exist!\n",
       "StackTrace: "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types will be printed.\n"
     ]
    }
   ],
   "source": [
    "%ShowTypes on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.linalg.distributed._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "import java.time.temporal.ChronoUnit\n",
    "import java.time.{Period, LocalDate, Instant}\n",
    "import java.sql.Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- To left align the HTML components in Markdown -->\n",
       "<style>\n",
       "table {float:left}\n",
       "</style>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html\n",
    "<!-- To left align the HTML components in Markdown -->\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark parition control based on core availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Uncaught exception: org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request! Cannot allocate containers as requested resource is greater than maximum allowed allocation. Requested resource type=[vcores], Requested resource=<memory:4505, vCores:8>, maximum allowed allocation=<memory:8192, vCores:4>, please note that maximum allowed allocation is calculated by scheduler based on maximum resource of registered NodeManagers, which might be less than configured maximum allocation=<memory:8192, vCores:4>\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.throwInvalidResourceException(SchedulerUtils.java:491)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.checkResourceRequestAgainstAvailableResource(SchedulerUtils.java:387)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:315)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:293)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:301)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.normalizeAndValidateRequests(RMServerUtils.java:250)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor.allocate(DefaultAMSProcessor.java:239)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor.allocate(DisabledPlacementProcessor.java:75)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain.allocate(AMSProcessingChain.java:92)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:424)\n",
       "\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)\n",
       "\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)\n",
       "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n",
       "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n",
       "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\n",
       "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\n",
       "\tat java.security.AccessController.doPrivileged(Native Method)\n",
       "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
       "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\n",
       "\n",
       "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
       "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
       "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
       "\tat org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)\n",
       "\tat org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:101)\n",
       "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:79)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n",
       "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n",
       "\tat com.sun.proxy.$Proxy14.allocate(Unknown Source)\n",
       "\tat org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:277)\n",
       "\tat org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:249)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster.createAllocator(ApplicationMaster.scala:450)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster.runExecutorLauncher(ApplicationMaster.scala:518)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster.org$apache$spark$deploy$yarn$ApplicationMaster$$runImpl(ApplicationMaster.scala:307)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$run$1.apply$mcV$sp(ApplicationMaster.scala:245)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$run$1.apply(ApplicationMaster.scala:245)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$run$1.apply(ApplicationMaster.scala:245)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:779)\n",
       "\tat java.security.AccessController.doPrivileged(Native Method)\n",
       "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
       "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster.doAsUser(ApplicationMaster.scala:778)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:244)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:803)\n",
       "\tat org.apache.spark.deploy.yarn.ExecutorLauncher$.main(ApplicationMaster.scala:833)\n",
       "\tat org.apache.spark.deploy.yarn.ExecutorLauncher.main(ApplicationMaster.scala)\n",
       "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException): Invalid resource request! Cannot allocate containers as requested resource is greater than maximum allowed allocation. Requested resource type=[vcores], Requested resource=<memory:4505, vCores:8>, maximum allowed allocation=<memory:8192, vCores:4>, please note that maximum allowed allocation is calculated by scheduler based on maximum resource of registered NodeManagers, which might be less than configured maximum allocation=<memory:8192, vCores:4>\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.throwInvalidResourceException(SchedulerUtils.java:491)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.checkResourceRequestAgainstAvailableResource(SchedulerUtils.java:387)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:315)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:293)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:301)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.normalizeAndValidateRequests(RMServerUtils.java:250)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor.allocate(DefaultAMSProcessor.java:239)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor.allocate(DisabledPlacementProcessor.java:75)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain.allocate(AMSProcessingChain.java:92)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:424)\n",
       "\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)\n",
       "\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)\n",
       "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n",
       "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n",
       "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\n",
       "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\n",
       "\tat java.security.AccessController.doPrivileged(Native Method)\n",
       "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
       "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\n",
       "\n",
       "\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\n",
       "\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n",
       "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n",
       "\tat com.sun.proxy.$Proxy13.allocate(Unknown Source)\n",
       "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)\n",
       "\t... 24 more\n",
       "\n",
       "StackTrace: \tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.throwInvalidResourceException(SchedulerUtils.java:491)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.checkResourceRequestAgainstAvailableResource(SchedulerUtils.java:387)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:315)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:293)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:301)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.normalizeAndValidateRequests(RMServerUtils.java:250)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor.allocate(DefaultAMSProcessor.java:239)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor.allocate(DisabledPlacementProcessor.java:75)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain.allocate(AMSProcessingChain.java:92)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:424)\n",
       "\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)\n",
       "\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)\n",
       "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n",
       "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n",
       "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\n",
       "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\n",
       "\tat java.security.AccessController.doPrivileged(Native Method)\n",
       "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
       "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\n",
       "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
       "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
       "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
       "\tat org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)\n",
       "\tat org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:101)\n",
       "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:79)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n",
       "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n",
       "\tat com.sun.proxy.$Proxy14.allocate(Unknown Source)\n",
       "\tat org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:277)\n",
       "\tat org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:249)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster.createAllocator(ApplicationMaster.scala:450)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster.runExecutorLauncher(ApplicationMaster.scala:518)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster.org$apache$spark$deploy$yarn$ApplicationMaster$$runImpl(ApplicationMaster.scala:307)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$run$1.apply$mcV$sp(ApplicationMaster.scala:245)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$run$1.apply(ApplicationMaster.scala:245)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$run$1.apply(ApplicationMaster.scala:245)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:779)\n",
       "\tat java.security.AccessController.doPrivileged(Native Method)\n",
       "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
       "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster.doAsUser(ApplicationMaster.scala:778)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:244)\n",
       "\tat org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:803)\n",
       "\tat org.apache.spark.deploy.yarn.ExecutorLauncher$.main(ApplicationMaster.scala:833)\n",
       "\tat org.apache.spark.deploy.yarn.ExecutorLauncher.main(ApplicationMaster.scala)\n",
       "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException): Invalid resource request! Cannot allocate containers as requested resource is greater than maximum allowed allocation. Requested resource type=[vcores], Requested resource=<memory:4505, vCores:8>, maximum allowed allocation=<memory:8192, vCores:4>, please note that maximum allowed allocation is calculated by scheduler based on maximum resource of registered NodeManagers, which might be less than configured maximum allocation=<memory:8192, vCores:4>\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.throwInvalidResourceException(SchedulerUtils.java:491)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.checkResourceRequestAgainstAvailableResource(SchedulerUtils.java:387)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:315)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:293)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:301)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.normalizeAndValidateRequests(RMServerUtils.java:250)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor.allocate(DefaultAMSProcessor.java:239)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor.allocate(DisabledPlacementProcessor.java:75)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain.allocate(AMSProcessingChain.java:92)\n",
       "\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:424)\n",
       "\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)\n",
       "\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)\n",
       "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n",
       "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n",
       "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\n",
       "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\n",
       "\tat java.security.AccessController.doPrivileged(Native Method)\n",
       "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
       "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\n",
       "\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\n",
       "\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n",
       "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n",
       "\tat com.sun.proxy.$Proxy13.allocate(Unknown Source)\n",
       "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)\n",
       "\t... 24 more\n",
       "  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.waitForApplication(YarnClientSchedulerBackend.scala:94)\n",
       "  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:63)\n",
       "  at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:183)\n",
       "  at org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n",
       "  at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)\n",
       "  at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:935)\n",
       "  at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:926)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)\n",
       "  at spark$lzycompute(<console>:57)\n",
       "  at spark(<console>:55)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NUM_CORES = 8\n",
    "val NUM_PARTITIONS = 3\n",
    "\n",
    "lazy val spark: SparkSession = SparkSession.builder()\n",
    "    .appName(\"mllib-rformula\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "/*\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.driver.memory\", \"6g\")\n",
    "spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "spark.conf.set(\"spark.master\", \"spark://masa:7077\")\n",
    "*/\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.IllegalStateException\n",
       "Message: Spark context stopped while waiting for backend\n",
       "StackTrace:   at org.apache.spark.scheduler.TaskSchedulerImpl.waitBackendReady(TaskSchedulerImpl.scala:818)\n",
       "  at org.apache.spark.scheduler.TaskSchedulerImpl.postStartHook(TaskSchedulerImpl.scala:196)\n",
       "  at org.apache.spark.SparkContext.<init>(SparkContext.scala:560)\n",
       "  at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)\n",
       "  at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:935)\n",
       "  at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:926)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)\n",
       "  at org.apache.toree.kernel.api.Kernel$$anonfun$1.apply(Kernel.scala:428)\n",
       "  at org.apache.toree.kernel.api.Kernel$$anonfun$1.apply(Kernel.scala:428)\n",
       "  at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
       "  at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
       "  at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121)\n",
       "  at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n",
       "  at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n",
       "  at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val configMap = spark.conf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROTOCOL: String = file://\n",
       "DATA_DIR: String = /home/oonisim/home/repositories/git/oonisim/spark-programs/Dataframe/data\n",
       "RESULT_DIR: String = .\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RESULT_DIR: String = .\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val PROTOCOL=\"file://\"\n",
    "val DATA_DIR=\"/home/oonisim/home/repositories/git/oonisim/spark-programs/Dataframe/data\"\n",
    "val RESULT_DIR=\".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var df = spark.read.json(PROTOCOL + DATA_DIR + \"/simple-ml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(\"value2\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val Array(train, test) = df.randomSplit(Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val rFormula = new RFormula()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "val lr = new LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(\n",
    "        rFormula,\n",
    "        lr\n",
    "    ))\n",
    "\n",
    "val model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "val params = new ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        rFormula.formula, \n",
    "        Array(\n",
    "            \"lab ~ . + color:value1\",\n",
    "            \"lab ~ . + color:value1 + color:value2\"\n",
    "        )\n",
    "    )\n",
    "    .addGrid(\n",
    "        lr.elasticNetParam, \n",
    "        Array(\n",
    "            0.0, 0.5, 1.0\n",
    "        )\n",
    "    )\n",
    "    .addGrid(\n",
    "        lr.regParam, \n",
    "        Array(\n",
    "            0.1, 2.0)\n",
    "    )\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "val evaluator = new BinaryClassificationEvaluator()\n",
    "  .setMetricName(\"areaUnderROC\")\n",
    "  .setRawPredictionCol(\"prediction\")\n",
    "  .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
    "val tvs = new TrainValidationSplit()\n",
    "  .setTrainRatio(0.75) // also the default.\n",
    "  .setEstimatorParamMaps(params)\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val tvsFitted = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(tvsFitted.transform(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

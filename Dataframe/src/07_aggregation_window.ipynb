{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 - Aggregation - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.linalg.distributed._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "import java.time.temporal.ChronoUnit\n",
    "import java.time.{Period, LocalDate, Instant}\n",
    "import java.sql.Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- To left align the HTML components in Markdown -->\n",
       "<style>\n",
       "table {float:left}\n",
       "</style>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html\n",
    "<!-- To left align the HTML components in Markdown -->\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark parition control based on core availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NUM_CORES = 2\n",
       "NUM_PARTITIONS = 2\n",
       "spark = <lazy>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<lazy>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NUM_CORES = 2\n",
    "val NUM_PARTITIONS = 2\n",
    "\n",
    "lazy val spark: SparkSession = SparkSession.builder()\n",
    "    .master(\"local\")\n",
    "    .appName(\"flight\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.default.parallelism\", 8)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "/*\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.driver.memory\", \"6g\")\n",
    "spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "spark.conf.set(\"spark.master\", \"spark://masa:7077\")\n",
    "*/\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.serializer,org.apache.spark.serializer.KryoSerializer)\n",
      "(spark.driver.host,172.17.0.1)\n",
      "(spark.eventLog.enabled,true)\n",
      "(spark.driver.port,46137)\n",
      "(spark.hadoop.validateOutputSpecs,True)\n",
      "(spark.repl.class.uri,spark://172.17.0.1:46137/classes)\n",
      "(spark.jars,file:/home/oonisim/.local/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.3.0-incubating.jar)\n",
      "(spark.repl.class.outputDir,/tmp/spark-70df602a-bc19-469d-806a-8ade41985168/repl-67091ce4-e778-456c-91e2-dae5be477de5)\n",
      "(spark.app.name,flight)\n",
      "(spark.driver.memory,2g)\n",
      "(spark.executor.instances,2)\n",
      "(spark.history.fs.logdirectory,hdfs://oonisim:8020/logs_spark)\n",
      "(spark.default.parallelism,8)\n",
      "(spark.executor.id,driver)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.master,local)\n",
      "(spark.executor.memory,4g)\n",
      "(spark.eventLog.dir,hdfs://oonisim:8020/logs_spark)\n",
      "(spark.executor.cores,4)\n",
      "(spark.app.id,local-1576109311900)\n",
      "(spark.sql.shuffle.partitions,4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "configMap: Unit = ()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val configMap = spark.conf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timing = \n",
       "times = ListBuffer()\n",
       "timed = > Nothing) => Nothing = <function2>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "clear: ()Unit\n",
       "average: ()Long\n",
       "_timed: [T](label: String, code: => T)T\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "> Nothing) => Nothing = <function2>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ListBuffer\n",
    "\n",
    "val timing = new StringBuffer\n",
    "val times = new ListBuffer[Long]()\n",
    "\n",
    "def clear(): Unit = {\n",
    "    timing.setLength(0)\n",
    "    times.clear\n",
    "}\n",
    "def average(): Long = {\n",
    "    times.reduce(_+_) / times.length\n",
    "}\n",
    "\n",
    "/**\n",
    "@param label Description about the run\n",
    "@code code to execute\n",
    "@return execution\n",
    "*/\n",
    "def _timed[T](label: String, code: => T): T = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val result = code\n",
    "    val stop = System.currentTimeMillis()\n",
    "    timing.append(s\"Processing $label took ${stop - start} ms.\\n\")\n",
    "    times.append(stop - start)\n",
    "    result\n",
    "}\n",
    "\n",
    "val timed = _timed _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:56: error: missing argument list for method _timed\n",
       "Unapplied methods are only converted to functions when a function type is expected.\n",
       "You can make this conversion explicit by writing `_timed _` or `_timed(_,_)` instead of `_timed`.\n",
       "       _timed\n",
       "       ^\n",
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// To flush out error: missing argument list for method timed\n",
    "println(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:59: error: not found: value RESULT_DIR\n",
       "           .save(RESULT_DIR)\n",
       "                 ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save(df: DataFrame) = {\n",
    "    df.coalesce(1)\n",
    "    .write\n",
    "    .format(\"csv\")\n",
    "    .mode(SaveMode.Overwrite)\n",
    "    .option(\"header\", \"true\")\n",
    "    .save(RESULT_DIR)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROTOCOL = file://\n",
       "HOME_DIR = /home/oonisim/home/repositories/git/oonisim/spark-programs/Dataframe\n",
       "df = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[InvoiceNo: string, StockCode: string ... 6 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val PROTOCOL=\"file://\"\n",
    "val HOME_DIR=\"/home/oonisim/home/repositories/git/oonisim/spark-programs/Dataframe\"\n",
    "val df = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(PROTOCOL + HOME_DIR + \"/data/retail-data/all/*.csv\")\n",
    "  .coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfWithDate = [InvoiceNo: string, StockCode: string ... 7 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[InvoiceNo: string, StockCode: string ... 7 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfWithDate = df.withColumn(\n",
    "    \"date\", \n",
    "    to_date(col(\"InvoiceDate\"),\"MM/d/yyyy H:mm\")\n",
    ")\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window\n",
    "Spark supports three kinds of window functions: ranking functions, analytic functions, and aggregate functions. The frame specification (the rowsBetween statement) states which rows will be included in the frame based on its reference to the current input row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "windowSpec = org.apache.spark.sql.expressions.WindowSpec@5fe51dcf\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.expressions.WindowSpec@5fe51dcf"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val windowSpec = Window\n",
    "  .partitionBy(\"CustomerId\", \"date\")\n",
    "  .orderBy(desc(\"Quantity\"))\n",
    "//  .rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "purchaseDenseRank = DENSE_RANK() OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST unspecifiedframe$())\n",
       "purchaseRank = RANK() OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST unspecifiedframe$())\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RANK() OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST unspecifiedframe$())"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "val purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|\n",
      "+----------+----------+--------+------------+-----------------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|\n",
      "|     12347|2010-12-07|      36|           1|                1|\n",
      "|     12347|2010-12-07|      30|           2|                2|\n",
      "|     12347|2010-12-07|      24|           3|                3|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|       6|          17|                5|\n",
      "|     12347|2010-12-07|       6|          17|                5|\n",
      "+----------+----------+--------+------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\n",
    "  .select(\n",
    "    col(\"CustomerId\"),\n",
    "    col(\"date\"),\n",
    "    col(\"Quantity\"),\n",
    "    purchaseRank.alias(\"quantityRank\"),\n",
    "    purchaseDenseRank.alias(\"quantityDenseRank\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollup\n",
    "An aggregation across multiple groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfNoNull = [InvoiceNo: string, StockCode: string ... 7 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[InvoiceNo: string, StockCode: string ... 7 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total & Sub Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|      Date|       Country|total_quantity|\n",
      "+----------+--------------+--------------+\n",
      "|      null|          null|       5176450|\n",
      "|2010-12-01|United Kingdom|         23949|\n",
      "|2010-12-01|       Germany|           117|\n",
      "|2010-12-01|     Australia|           107|\n",
      "|2010-12-01|          EIRE|           243|\n",
      "|2010-12-01|        France|           449|\n",
      "|2010-12-01|        Norway|          1852|\n",
      "|2010-12-01|   Netherlands|            97|\n",
      "|2010-12-01|          null|         26814|\n",
      "|2010-12-02|          null|         21023|\n",
      "|2010-12-02|          EIRE|             4|\n",
      "|2010-12-02|United Kingdom|         20873|\n",
      "|2010-12-02|       Germany|           146|\n",
      "|2010-12-03|United Kingdom|         10439|\n",
      "|2010-12-03|          null|         14830|\n",
      "|2010-12-03|         Italy|           164|\n",
      "|2010-12-03|       Germany|           170|\n",
      "|2010-12-03|   Switzerland|           110|\n",
      "|2010-12-03|       Belgium|           528|\n",
      "|2010-12-03|        Poland|           140|\n",
      "+----------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rolledUpDF = [Date: date, Country: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Date: date, Country: string ... 1 more field]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\n",
    "  .selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\")\n",
    "  .orderBy(\"Date\")\n",
    "rolledUpDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cube\n",
    "Disect the entire data with all the combinations of specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------+\n",
      "|      Date|Country|sum(Quantity)|\n",
      "+----------+-------+-------------+\n",
      "|      null|Denmark|         8188|\n",
      "|2011-02-16|Denmark|          224|\n",
      "|2011-06-23|Denmark|          358|\n",
      "|2011-08-10|Denmark|          338|\n",
      "|2011-09-26|Denmark|          230|\n",
      "|2011-09-29|Denmark|          202|\n",
      "|2011-10-11|Denmark|          136|\n",
      "|2011-05-11|Denmark|          368|\n",
      "|2011-11-17|Denmark|         1025|\n",
      "|2010-12-09|Denmark|          454|\n",
      "|2011-03-17|Denmark|          879|\n",
      "|2011-06-20|Denmark|          393|\n",
      "|2011-07-25|Denmark|          241|\n",
      "|2011-08-09|Denmark|           -9|\n",
      "|2011-09-21|Denmark|         1176|\n",
      "|2011-10-17|Denmark|          -37|\n",
      "|2011-11-30|Denmark|          512|\n",
      "|2011-06-09|Denmark|          793|\n",
      "|2011-10-07|Denmark|          637|\n",
      "|2011-10-21|Denmark|           95|\n",
      "+----------+-------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Sum of quantity with all (date, country) combination as well as over all dates.\n",
    "dfNoNull.cube(\"Date\", \"Country\").agg(sum(col(\"Quantity\")))\n",
    "    .where(col(\"Country\") === \"Denmark\")\n",
    "    .select(\"Date\", \"Country\", \"sum(Quantity)\")\n",
    "    .orderBy(expr(\"grouping_id()\").desc)\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping ID\n",
    "Grouping ID\tDescription  \n",
    "0: Aggregatino at combination of each columns (date, country)\n",
    "1. Aggregation at the 1st column (date)\n",
    "2. Aggregation at the 2nd column (ountry)\n",
    "3. Total aggregation regardless columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------+-------------+\n",
      "|      Date|Country|grouping_id()|sum(Quantity)|\n",
      "+----------+-------+-------------+-------------+\n",
      "|      null|Denmark|            2|         8188|\n",
      "|2011-10-11|Denmark|            0|          136|\n",
      "|2011-06-23|Denmark|            0|          358|\n",
      "|2011-08-10|Denmark|            0|          338|\n",
      "|2011-09-26|Denmark|            0|          230|\n",
      "+----------+-------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNoNull\n",
    "    .cube(\"Date\", \"Country\")\n",
    "    .agg(\n",
    "        grouping_id(),\n",
    "        sum(col(\"Quantity\"))\n",
    "    )\n",
    "    .where(col(\"Country\") === \"Denmark\")\n",
    "    .select(\"Date\", \"Country\", \"grouping_id()\", \"sum(Quantity)\")\n",
    "    .orderBy(expr(\"grouping_id()\").desc)\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------+-------------+\n",
      "|      Date|Country|grouping_id()|sum(Quantity)|\n",
      "+----------+-------+-------------+-------------+\n",
      "|2011-10-11|   null|            1|        20091|\n",
      "|2011-10-11|Belgium|            0|          162|\n",
      "|2011-10-11| France|            0|         2788|\n",
      "|2011-10-11|Germany|            0|          567|\n",
      "|2011-10-11|   EIRE|            0|         2206|\n",
      "+----------+-------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNoNull\n",
    "    .cube(\"Date\", \"Country\")\n",
    "    .agg(\n",
    "        grouping_id(),\n",
    "        sum(col(\"Quantity\"))\n",
    "    )\n",
    "    .where(col(\"Date\") === \"2011-10-11\")\n",
    "    .select(\"Date\", \"Country\", \"grouping_id()\", \"sum(Quantity)\")\n",
    "    .orderBy(expr(\"grouping_id()\").desc)\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class BoolAnd\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "class BoolAnd extends UserDefinedAggregateFunction {\n",
    "  def inputSchema: org.apache.spark.sql.types.StructType =\n",
    "    StructType(StructField(\"value\", BooleanType) :: Nil)\n",
    "  def bufferSchema: StructType = StructType(\n",
    "    StructField(\"result\", BooleanType) :: Nil\n",
    "  )\n",
    "  def dataType: DataType = BooleanType\n",
    "  def deterministic: Boolean = true\n",
    "  def initialize(buffer: MutableAggregationBuffer): Unit = {\n",
    "    buffer(0) = true\n",
    "  }\n",
    "  def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n",
    "    buffer(0) = buffer.getAs[Boolean](0) && input.getAs[Boolean](0)\n",
    "  }\n",
    "  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n",
    "    buffer1(0) = buffer1.getAs[Boolean](0) && buffer2.getAs[Boolean](0)\n",
    "  }\n",
    "  def evaluate(buffer: Row): Any = {\n",
    "    buffer(0)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|booland(t)|booland(f)|\n",
      "+----------+----------+\n",
      "|      true|     false|\n",
      "+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ba = BoolAnd@5eceb18b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BoolAnd@5eceb18b"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ba = new BoolAnd\n",
    "spark.udf.register(\"booland\", ba)\n",
    "import org.apache.spark.sql.functions._\n",
    "spark.range(1)\n",
    "  .selectExpr(\"explode(array(TRUE, TRUE, TRUE)) as t\")\n",
    "  .selectExpr(\"explode(array(TRUE, FALSE, TRUE)) as f\", \"t\")\n",
    "  .select(ba(col(\"t\")), expr(\"booland(f)\"))\n",
    "  .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

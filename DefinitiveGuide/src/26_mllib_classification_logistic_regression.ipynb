{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 26 - Classification - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types will be printed.\n"
     ]
    }
   ],
   "source": [
    "%ShowTypes on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification._\n",
    "\n",
    "import java.time.{Period, LocalDate, Instant}\n",
    "import java.sql.Timestamp\n",
    "import java.time.temporal.ChronoUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- To left align the HTML components in Markdown -->\n",
       "<style>\n",
       "table {float:left}\n",
       "</style>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html\n",
    "<!-- To left align the HTML components in Markdown -->\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark parition control based on core availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NUM_CORES: Int = 4\n",
       "NUM_PARTITIONS: Int = 4\n",
       "spark: org.apache.spark.sql.SparkSession = <lazy>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = <lazy>\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NUM_CORES = 4\n",
    "val NUM_PARTITIONS = 4\n",
    "\n",
    "lazy val spark: SparkSession = SparkSession.builder()\n",
    "    .appName(\"mllib-cross-validation\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "/*\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.driver.memory\", \"6g\")\n",
    "spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "spark.conf.set(\"spark.master\", \"spark://masa:7077\")\n",
    "*/\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.serializer,org.apache.spark.serializer.KryoSerializer)\n",
      "(spark.driver.host,172.19.210.168)\n",
      "(spark.eventLog.enabled,true)\n",
      "(spark.driver.port,46361)\n",
      "(spark.hadoop.validateOutputSpecs,True)\n",
      "(spark.repl.class.uri,spark://172.19.210.168:46361/classes)\n",
      "(spark.jars,file:/home/oonisim/.local/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.3.0-incubating.jar)\n",
      "(spark.repl.class.outputDir,/tmp/spark-ba348052-5850-48b9-84b2-91a8da343c06/repl-1c601d40-eff9-40ce-92e5-da71ab984df5)\n",
      "(spark.app.name,mllib-cross-validation)\n",
      "(spark.driver.memory,3g)\n",
      "(spark.executor.instances,2)\n",
      "(spark.history.fs.logdirectory,hdfs://oonisim:8020/logs_spark)\n",
      "(spark.default.parallelism,16)\n",
      "(spark.executor.id,driver)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.master,yarn)\n",
      "(spark.ui.filters,org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter)\n",
      "(spark.executor.memory,4g)\n",
      "(spark.eventLog.dir,hdfs://oonisim:8020/logs_spark)\n",
      "(spark.executor.cores,4)\n",
      "(spark.driver.appUIAddress,http://172.19.210.168:4040)\n",
      "(spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS,oonisim)\n",
      "(spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES,http://oonisim:8088/proxy/application_1576535603499_0001)\n",
      "(spark.app.id,application_1576535603499_0001)\n",
      "(spark.sql.shuffle.partitions,16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "configMap: Unit = ()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val configMap = spark.conf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROTOCOL: String = file://\n",
       "DATA_DIR: String = /home/oonisim/home/repositories/git/oonisim/spark-programs/Dataframe/data\n",
       "RESULT_DIR: String = .\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RESULT_DIR: String = .\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val PROTOCOL=\"file://\"\n",
    "val DATA_DIR=\"/home/oonisim/home/repositories/git/oonisim/spark-programs/Dataframe/data\"\n",
    "val RESULT_DIR=\".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bInput: org.apache.spark.sql.DataFrame = [features: vector, label: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "bInput: org.apache.spark.sql.DataFrame = [features: vector, label: double]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bInput = spark.read.format(\"parquet\").load(PROTOCOL + DATA_DIR + \"/binary-classification\")\n",
    "  .selectExpr(\"features\", \"cast(label as double) as label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      features|label|\n",
      "+--------------+-----+\n",
      "|[3.0,10.1,3.0]|  1.0|\n",
      "|[1.0,0.1,-1.0]|  0.0|\n",
      "|[1.0,0.1,-1.0]|  0.0|\n",
      "+--------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bInput.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\n",
      "featuresCol: features column name (default: features)\n",
      "fitIntercept: whether to fit an intercept term (default: true)\n",
      "labelCol: label column name (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model (default: true)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-6)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_ba88cd353c1e\n",
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_ba88cd353c1e, numClasses = 2, numFeatures = 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_ba88cd353c1e, numClasses = 2, numFeatures = 3\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression()\n",
    "println(lr.explainParams()) // see all parameters\n",
    "val lrModel = lr.fit(bInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Weights of the features (coefficients) and intercept. For a multinomial model (the current one is binary), lrModel.coefficientMatrix and lrModel.interceptVecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.848741326855034,0.35356589010197487,14.814900276915889]\n",
      "-10.225695864480993\n"
     ]
    }
   ],
   "source": [
    "println(lrModel.coefficients)\n",
    "println(lrModel.intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance\n",
    "Note that for the area under the curve, instance weighting is not taken into account, so if you wanted to see how you performed on the values you weighed more highly, you’d have to do that manually. This will probably change in future Spark versions.\n",
    "\n",
    "The model summary is currently only available for binary logistic regression problems, but multiclass summaries will likely be added in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "+---+------------------+\n",
      "|FPR|               TPR|\n",
      "+---+------------------+\n",
      "|0.0|               0.0|\n",
      "|0.0|0.3333333333333333|\n",
      "|0.0|               1.0|\n",
      "|1.0|               1.0|\n",
      "|1.0|               1.0|\n",
      "+---+------------------+\n",
      "\n",
      "+------------------+---------+\n",
      "|            recall|precision|\n",
      "+------------------+---------+\n",
      "|               0.0|      1.0|\n",
      "|0.3333333333333333|      1.0|\n",
      "|               1.0|      1.0|\n",
      "|               1.0|      0.6|\n",
      "+------------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary: org.apache.spark.ml.classification.LogisticRegressionTrainingSummary = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummaryImpl@25c6e15f\n",
       "bSummary: org.apache.spark.ml.classification.BinaryLogisticRegressionSummary = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummaryImpl@25c6e15f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "bSummary: org.apache.spark.ml.classification.BinaryLogisticRegressionSummary = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummaryImpl@25c6e15f\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val summary = lrModel.summary\n",
    "val bSummary = summary.asInstanceOf[BinaryLogisticRegressionSummary]\n",
    "println(bSummary.areaUnderROC)\n",
    "bSummary.roc.show()\n",
    "bSummary.pr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion Rate\n",
    "The speed at which the model descends to the final result is shown in the objective history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6730116670092565\n",
      "0.5042829330409728\n",
      "0.36356862066874396\n",
      "0.1252407018038338\n",
      "0.08532556611276212\n",
      "0.03550487641573043\n",
      "0.01819649450857124\n",
      "0.008817369922959128\n",
      "0.004413673785392138\n",
      "0.002194038351234706\n",
      "0.001096564114808084\n",
      "5.476575519853136E-4\n",
      "2.73762379514901E-4\n",
      "1.368465223657475E-4\n",
      "6.84180903707058E-5\n",
      "3.4207077910384856E-5\n",
      "1.710317666423191E-5\n",
      "8.551470106426885E-6\n",
      "4.275703677941412E-6\n",
      "2.1378240117781396E-6\n",
      "1.0688564054651744E-6\n",
      "5.342600202575258E-7\n",
      "2.668135105897087E-7\n",
      "1.32046278653136E-7\n",
      "6.768401481681801E-8\n",
      "3.3145477184834547E-8\n",
      "1.6151438837488498E-8\n",
      "8.309350118268437E-9\n"
     ]
    }
   ],
   "source": [
    "summary.objectiveHistory.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

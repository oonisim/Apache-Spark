{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 03\n",
    "Find the greatest number of countries a passenger has been in without being in the UK. For example, if the countries a passenger was in were: UK -> FR -> US -> CN -> UK -> DE -> UK, the correct answer would be 3 countries.\n",
    "\n",
    "## Assumptions\n",
    "1. Run length is in-between UK and ends with UK. Do not count a run that does not starts or ends with UK.\n",
    "2. Data is clearned and not errorneous\n",
    "3. Timezone consideration is not required\n",
    "\n",
    "## Approaches\n",
    "\n",
    "1. SQL window function (row_number)  \n",
    "    a. Add row numbers over each passenger partition that has been orderd by date.  \n",
    "    b. Identify matching depart-from UK (+1), and return-to UK (-1) rows, and others (0).  \n",
    "    c. Remove rows that is neither depart nor return -> marked as 0.  \n",
    "    d. Get the row number differnce between (depart, return) rows. <br/><br/>  \n",
    "\n",
    "2. RDD groupByKey & map  \n",
    "    a. Generate a string of flight-run for each passengerId e.g. \"JP KR **UK** FR US CN **UK** DE **UK** CR TH\".   \n",
    "    b. Extract each match with a regexp '**UK** (.+) **UK**'.  \n",
    "    c. Find the longest one.   \n",
    "\n",
    "groupByKey does not preserve order, hence need to insert a row number within a passengerId partition in the (k,v) pair where v is (row_number, country)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.time.temporal.ChronoUnit\n",
    "import java.time.{Period, LocalDate, Instant}\n",
    "import java.sql.Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark parition control based on core availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NUM_CORES = 4\n",
       "NUM_PARTITIONS = 3\n",
       "spark = <lazy>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<lazy>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NUM_CORES = 4\n",
    "val NUM_PARTITIONS = 3\n",
    "\n",
    "lazy val spark: SparkSession = SparkSession.builder()\n",
    "    .master(\"local\")\n",
    "    .appName(\"flight\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FLIGHTDATA_CSV_PATH = ../resources/flightData.csv\n",
       "PASSENGER_CSV_PATH = ../resources/passengers.csv\n",
       "RESULT_DIR = results/longestRun\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "results/longestRun"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val FLIGHTDATA_CSV_PATH = \"../resources/flightData.csv\"\n",
    "val PASSENGER_CSV_PATH = \"../resources/passengers.csv\"\n",
    "val RESULT_DIR = \"results/longestRun\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timing = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "timed: [T](label: String, code: => T)T\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val timing = new StringBuffer\n",
    "def timed[T](label: String, code: => T): T = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val result = code\n",
    "    val stop = System.currentTimeMillis()\n",
    "    timing.append(s\"Processing $label took ${stop - start} ms.\\n\")\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:46: error: missing argument list for method timed\n",
       "Unapplied methods are only converted to functions when a function type is expected.\n",
       "You can make this conversion explicit by writing `timed _` or `timed(_,_)` instead of `timed`.\n",
       "       timed\n",
       "       ^\n",
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// To flush out error: missing argument list for method timed\n",
    "println(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF\n",
    "Get monthes between dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BASE_LOCALDATE = 2017-01-01\n",
       "udf_months_between = UserDefinedFunction(<function1>,ShortType,Some(List(TimestampType)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "get_months_between: (to: java.sql.Timestamp)Short\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,ShortType,Some(List(TimestampType)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val BASE_TIMESTAMP = java.sql.Timestamp.valueOf(\"2017-01-01 00:00:00.0\")\n",
    "val BASE_LOCALDATE = LocalDate.parse(\"2017-01-01\").withDayOfMonth(1)\n",
    "\n",
    "def get_months_between(to: Timestamp): Short = {\n",
    "    val monthsBetween = ChronoUnit.MONTHS.between(\n",
    "        BASE_LOCALDATE,\n",
    "        to.toLocalDateTime().toLocalDate().withDayOfMonth(1)\n",
    "    )\n",
    "    monthsBetween.toShort\n",
    "}\n",
    "val udf_months_between = udf((t:Timestamp) => get_months_between(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base DataFrame\n",
    "* Mark depart from UK as +1, return to UK as -1, and else as 0.  \n",
    "* Sort by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passengerId: integer (nullable = true)\n",
      " |-- flightId: integer (nullable = true)\n",
      " |-- from: string (nullable = true)\n",
      " |-- to: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- direction: integer (nullable = false)\n",
      " |-- count: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightData = [passengerId: int, flightId: int ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 5 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transformations, no action yet\n",
    "val flightData = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"../resources/flightData.csv\")\n",
    "    .withColumn(\n",
    "        \"direction\", \n",
    "        when(lower(col(\"from\")) === \"uk\", 1)\n",
    "        .when(lower(col(\"to\"))   === \"uk\", -1)\n",
    "        .otherwise(0)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"count\", lit(1)\n",
    "    )\n",
    "    .orderBy(asc(\"passengerId\"), asc(\"date\"))\n",
    "\n",
    "flightData.printSchema()\n",
    "flightData.createOrReplaceTempView(\"flightData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row number\n",
    "Add row-number within each passernerId partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "querySequencedRun = \n",
       "sequencedRun = [passengerId: int, flightId: int ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "SELECT\n",
       "    f.*,\n",
       "    ROW_NUMBER() OVER (PARTITION BY passengerId ORDER BY passengerId, date) as seq\n",
       "FROM\n",
       "    flightData f\n",
       "ORDER BY\n",
       "    passengerId, date\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 6 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val querySequencedRun = \"\"\"\n",
    "SELECT \n",
    "    f.*,\n",
    "    ROW_NUMBER() OVER (PARTITION BY passengerId ORDER BY passengerId, date) as seq \n",
    "FROM\n",
    "    flightData f\n",
    "ORDER BY \n",
    "    passengerId, date\n",
    "\"\"\"\n",
    "\n",
    "val sequencedRun = spark.sql(querySequencedRun)\n",
    "sequencedRun.createOrReplaceTempView(\"sequencedRun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longest run per passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "queryLongestRun = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "WITH\n",
       "    closedRun AS (\n",
       "        SELECT\n",
       "            passengerId,\n",
       "            from, to,\n",
       "            direction,\n",
       "            seq,\n",
       "            --------------------------------------------------------------------------------\n",
       "            -- For a departure flight, take the the return flight, if there is, seq num\n",
       "            --------------------------------------------------------------------------------\n",
       "            CASE\n",
       "                WHEN direction == 1\n",
       "                THEN lead(seq) OVER (PARTITION BY passengerId ORDER BY seq)\n",
       "            END AS return,\n",
       "            --------------------------------------------------------------------------------\n",
       "            -- For a departure flight, count the visiting countries, if returned.\n",
       "            --------------------------...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val queryLongestRun = \"\"\"\n",
    "WITH \n",
    "    closedRun AS (\n",
    "        SELECT \n",
    "            passengerId, \n",
    "            from, to, \n",
    "            direction, \n",
    "            seq,\n",
    "            -------------------------------------------------------------------------------- \n",
    "            -- For a departure flight, take the the return flight, if there is, seq num\n",
    "            -------------------------------------------------------------------------------- \n",
    "            CASE \n",
    "                WHEN direction == 1\n",
    "                THEN lead(seq) OVER (PARTITION BY passengerId ORDER BY seq)\n",
    "            END AS return,\n",
    "            -------------------------------------------------------------------------------- \n",
    "            -- For a departure flight, count the visiting countries, if returned.\n",
    "            -------------------------------------------------------------------------------- \n",
    "            CASE \n",
    "                WHEN direction == 1\n",
    "                THEN lead(seq) OVER (PARTITION BY passengerId ORDER BY seq) - seq\n",
    "            END AS countries\n",
    "        FROM sequencedRun s\n",
    "        WHERE \n",
    "            -------------------------------------------------------------------------------- \n",
    "            -- Remove those without UK\n",
    "            -------------------------------------------------------------------------------- \n",
    "            direction != 0\n",
    "            -------------------------------------------------------------------------------- \n",
    "            -- Select passengers having both depart (+1) and return (-1), which is \n",
    "            -- distinct direction count is 2.\n",
    "            -------------------------------------------------------------------------------- \n",
    "            AND EXISTS (  \n",
    "                SELECT passengerId\n",
    "                FROM\n",
    "                    sequencedRun\n",
    "                WHERE \n",
    "                    direction != 0 AND\n",
    "                    passengerId == s.passengerId\n",
    "                GROUP BY\n",
    "                    passengerId\n",
    "                Having count(DISTINCT direction) == 2\n",
    "            )\n",
    "        ORDER BY \n",
    "            passengerId, seq\n",
    "    )\n",
    "    \n",
    "\n",
    "SELECT \n",
    "    passengerId as `Passenger ID`,\n",
    "    max(countries) as `Longest Run`\n",
    "FROM closedRun\n",
    "WHERE \n",
    "    countries IS NOT NULL\n",
    "GROUP BY \n",
    "    passengerId\n",
    "ORDER BY \n",
    "    max(countries) DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|Passenger ID|Longest Run|\n",
      "+------------+-----------+\n",
      "|        2975|         16|\n",
      "|        2939|         15|\n",
      "|        8562|         15|\n",
      "|         760|         15|\n",
      "|        3573|         15|\n",
      "+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Run longest closed run. took 6119 ms.\n",
      "\n",
      "(12) MapPartitionsRDD[78] at rdd at <console>:59 []\n",
      " |   MapPartitionsRDD[77] at rdd at <console>:59 []\n",
      " |   MapPartitionsRDD[76] at rdd at <console>:59 []\n",
      " |   ShuffledRowRDD[75] at rdd at <console>:59 []\n",
      " +-(12) MapPartitionsRDD[74] at rdd at <console>:59 []\n",
      "    |   MapPartitionsRDD[70] at rdd at <console>:59 []\n",
      "    |   MapPartitionsRDD[69] at rdd at <console>:59 []\n",
      "    |   ShuffledRowRDD[68] at rdd at <console>:59 []\n",
      "    +-(12) MapPartitionsRDD[67] at rdd at <console>:59 []\n",
      "       |   MapPartitionsRDD[66] at rdd at <console>:59 []\n",
      "       |   ShuffledRowRDD[65] at rdd at <console>:59 []\n",
      "       +-(1) MapPartitionsRDD[64] at rdd at <console>:59 []\n",
      "          |  MapPartitionsRDD[60] at rdd at <console>:59 []\n",
      "          |  FileScanRDD[59] at rdd at <console>:59 []\n",
      "(8) MapPartitionsRDD[134] at rdd at <console>:60 []\n",
      " |  MapPartitionsRDD[133] at rdd at <console>:60 []\n",
      " |  MapPartitionsRDD[132] at rdd at <console>:60 []\n",
      " |  ShuffledRowRDD[131] at rdd at <console>:60 []\n",
      " +-(12) MapPartitionsRDD[130] at rdd at <console>:60 []\n",
      "    |   MapPartitionsRDD[126] at rdd at <console>:60 []\n",
      "    |   ShuffledRowRDD[125] at rdd at <console>:60 []\n",
      "    +-(12) MapPartitionsRDD[124] at rdd at <console>:60 []\n",
      "       |   MapPartitionsRDD[123] at rdd at <console>:60 []\n",
      "       |   ShuffledRowRDD[122] at rdd at <console>:60 []\n",
      "       +-(12) MapPartitionsRDD[121] at rdd at <console>:60 []\n",
      "          |   MapPartitionsRDD[117] at rdd at <console>:60 []\n",
      "          |   MapPartitionsRDD[116] at rdd at <console>:60 []\n",
      "          |   MapPartitionsRDD[115] at rdd at <console>:60 []\n",
      "          |   ShuffledRowRDD[114] at rdd at <console>:60 []\n",
      "          +-(12) MapPartitionsRDD[113] at rdd at <console>:60 []\n",
      "             |   MapPartitionsRDD[112] at rdd at <console>:60 []\n",
      "             |   ShuffledRowRDD[111] at rdd at <console>:60 []\n",
      "             +-(12) MapPartitionsRDD[110] at rdd at <console>:60 []\n",
      "                |   MapPartitionsRDD[106] at rdd at <console>:60 []\n",
      "                |   MapPartitionsRDD[105] at rdd at <console>:60 []\n",
      "                |   MapPartitionsRDD[104] at rdd at <console>:60 []\n",
      "                |   ShuffledRowRDD[103] at rdd at <console>:60 []\n",
      "                +-(12) MapPartitionsRDD[102] at rdd at <console>:60 []\n",
      "                   |   MapPartitionsRDD[101] at rdd at <console>:60 []\n",
      "                   |   ShuffledRowRDD[100] at rdd at <console>:60 []\n",
      "                   +-(1) MapPartitionsRDD[99] at rdd at <console>:60 []\n",
      "                      |  MapPartitionsRDD[95] at rdd at <console>:60 []\n",
      "                      |  FileScanRDD[94] at rdd at <console>:60 []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "longestRun = [Passenger ID: int, Longest Run: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Passenger ID: int, Longest Run: int]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val longestRun = spark.sql(queryLongestRun)\n",
    "\n",
    "timed(\n",
    "    \"Run longest closed run.\",\n",
    "    longestRun.show(5)\n",
    ")\n",
    "println(timing)\n",
    "println(sequencedRun.rdd.toDebugString)\n",
    "println(longestRun.rdd.toDebugString)\n",
    "\n",
    "longestRun\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .format(\"csv\")\n",
    "    .mode(SaveMode.Overwrite)\n",
    "    .option(\"header\", \"true\")\n",
    "    .save(RESULT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validations\n",
    "Simple tests.\n",
    "\n",
    "## TBD \n",
    "Proper tests.\n",
    "\n",
    "### Test cases\n",
    "#### Normal cases\n",
    "1. Passenger without UK -> .. -> UK  \n",
    "    a. Run length is 1  \n",
    "    b. Run length > 1    \n",
    "<br/>\n",
    "\n",
    "2. Passenger with UK  \n",
    "    a. Run with depart-from UK only  \n",
    "    b. Run with return-to UK only  \n",
    "    c. Run with only one (return, depart)  \n",
    "    d. Run with only one (depart, return)  \n",
    "    e. Run with return, (depart, return)+  \n",
    "    f. Run with (depart, return)+, depart  \n",
    "    f. Run with (depart, return)+  \n",
    "\n",
    "#### Error cases\n",
    "TBD e.g. (return-to UK, return-to UK) without depart in-between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ cat ../../../main/resources/flightData.csv | awk '{FS=\",\"} /^53,/{print $3,$4, $5}'  | sort -k3\n",
    "cg ir 2017-01-01\n",
    "ir sg 2017-01-10\n",
    "sg nl 2017-01-29\n",
    "nl at 2017-02-13\n",
    "at ch 2017-03-27\n",
    "ch uk 2017-04-07 < Return\n",
    "uk se 2017-04-10 > Depart\n",
    "se uk 2017-04-23 < Return\n",
    "uk tj 2017-05-26 > Depart\n",
    "tj fr 2017-05-29\n",
    "fr pk 2017-06-03\n",
    "pk th 2017-06-04\n",
    "th uk 2017-06-14 <---- Return\n",
    "\n",
    "$ cat ../../../main/resources/flightData.csv | awk '{FS=\",\"} /^227,/{print $3,$4, $5}'  | sort -k3\n",
    "ca cn 2017-01-01\n",
    "cn at 2017-01-13\n",
    "at pk 2017-01-17 \n",
    "pk iq 2017-03-29\n",
    "iq uk 2017-04-11 < (no matching depart)\n",
    "uk uk 2017-05-11 > Depart\n",
    "uk ca 2017-07-24 < Return\n",
    "ca cn 2017-08-06\n",
    "cn bm 2017-08-16\n",
    "bm iq 2017-10-04\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

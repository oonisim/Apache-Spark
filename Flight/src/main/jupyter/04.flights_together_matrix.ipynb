{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 04\n",
    "Find the passengers who have been on more than N flights together within the range (from,to).\n",
    "\n",
    "## Assumptions\n",
    "1. \"more than 3 flights together\" means 4, 5, ... which is bigger than 3. \n",
    "2. Data is clearned and not errorneous\n",
    "3. Timezone consideration is not required\n",
    "\n",
    "## Approaches\n",
    "Have a matrix M where row is passenger and column is flight. The diagonal of the product **M * M<sup>T</sup>** represents the number of total flghts of each passenger, and right top part represents how many flights he/she shared with other passengers, e.g. (row/passenger A, column/passenger B) shows the number of flights that passenger A shared with passenger B.\n",
    "\n",
    "<img align=\"left\" src=\"./images/matrix.png\" width=\"1000\">  \n",
    "<br/>\n",
    "\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations\n",
    "\n",
    "For product operation of Matrix, Spark may convert sparse matrix into dense, which causes out of memory exceptions for large matrices. \n",
    "\n",
    "Spark Matrix does not preserve the order of rows unless using IndexedRow matrix.\n",
    "\n",
    "For scalability, consider adopting Map/Reduce with CoodinateMatrix as in [Scalable Sparse Matrix Multiplication in Apache Spark](https://medium.com/balabit-unsupervised/scalable-sparse-matrix-multiplication-in-apache-spark-c79e9ffc0703).\n",
    "\n",
    "> Multiply matrices in the MapReduce way in the book Mining of Massive Datasets by Leskovec, Rajaraman and Ullman. \n",
    "\n",
    "```\n",
    "def coordinateMatrixMultiply(\n",
    "    leftMatrix: CoordinateMatrix,\n",
    "    rightMatrix: CoordinateMatrix): CoordinateMatrix = {\n",
    "  \n",
    "  val M_ = leftMatrix.entries.map({ case MatrixEntry(i, j, v) => (j, (i, v)) })\n",
    "  val N_ = rightMatrix.entries.map({ case MatrixEntry(j, k, w) => (j, (k, w)) })\n",
    "  val productEntries = M_\n",
    "    .join(N_)\n",
    "    .map({ case (_, ((i, v), (k, w))) => ((i, k), (v * w)) })\n",
    "    .reduceByKey(_ + _)\n",
    "    .map({ case ((i, k), sum) => MatrixEntry(i, k, sum) })\n",
    "  new CoordinateMatrix(productEntries)\n",
    "}\n",
    "```\n",
    "\n",
    "## Output\n",
    "\n",
    "To avoid having duplicates, passegner 1 ID < passenger ID 2. For instance there will be no (0, 1, 14) and (1, 0, 14) togeher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.linalg.distributed._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "import java.time.temporal.ChronoUnit\n",
    "import java.time.{Period, LocalDate, Instant}\n",
    "import java.sql.Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark parition control based on core availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NUM_CORES = 2\n",
       "NUM_PARTITIONS = 3\n",
       "spark = <lazy>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<lazy>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NUM_CORES = 4\n",
    "val NUM_PARTITIONS = 2\n",
    "\n",
    "lazy val spark: SparkSession = SparkSession.builder()\n",
    "    .appName(\"flight_together_matrix\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "/*\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.driver.memory\", \"6g\")\n",
    "spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "spark.conf.set(\"spark.master\", \"spark://masa:7077\")\n",
    "*/\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "configMap = Map(spark.serializer -> org.apache.spark.serializer.KryoSerializer, spark.driver.host -> 10.186.87.0, spark.eventLog.enabled -> true, spark.driver.port -> 41269, spark.hadoop.validateOutputSpecs -> True, spark.repl.class.uri -> spark://10.186.87.0:41269/classes, spark.jars -> file:/home/oonisim/.local/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.3.0-incubating.jar, spark.repl.class.outputDir -> /tmp/spark-5bac9a21-f854-4b4a-ac2a-418e86494eb1/repl-29dc526f-f4e3-48aa-8bec-41c285ca7102, spark.app.name -> flight_together_matrix, spark.driver.memory -> 2g, spark.executor.instances -> 2, spark.history.fs.logdirectory -> hdfs://oonisim:8020/logs_spark, spark.default.parallelism -> 6, spark.executor.id -> driver, spark.submit.deployMode -> client...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(spark.serializer -> org.apache.spark.serializer.KryoSerializer, spark.driver.host -> 10.186.87.0, spark.eventLog.enabled -> true, spark.driver.port -> 41269, spark.hadoop.validateOutputSpecs -> True, spark.repl.class.uri -> spark://10.186.87.0:41269/classes, spark.jars -> file:/home/oonisim/.local/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.3.0-incubating.jar, spark.repl.class.outputDir -> /tmp/spark-5bac9a21-f854-4b4a-ac2a-418e86494eb1/repl-29dc526f-f4e3-48aa-8bec-41c285ca7102, spark.app.name -> flight_together_matrix, spark.driver.memory -> 2g, spark.executor.instances -> 2, spark.history.fs.logdirectory -> hdfs://oonisim:8020/logs_spark, spark.default.parallelism -> 6, spark.executor.id -> driver, spark.submit.deployMode -> client..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val configMap = spark.conf.getAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.serializer,org.apache.spark.serializer.KryoSerializer)\n",
      "(spark.driver.host,10.186.87.0)\n",
      "(spark.eventLog.enabled,true)\n",
      "(spark.driver.port,41269)\n",
      "(spark.hadoop.validateOutputSpecs,True)\n",
      "(spark.repl.class.uri,spark://10.186.87.0:41269/classes)\n",
      "(spark.jars,file:/home/oonisim/.local/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.3.0-incubating.jar)\n",
      "(spark.repl.class.outputDir,/tmp/spark-5bac9a21-f854-4b4a-ac2a-418e86494eb1/repl-29dc526f-f4e3-48aa-8bec-41c285ca7102)\n",
      "(spark.app.name,flight_together_matrix)\n",
      "(spark.driver.memory,2g)\n",
      "(spark.executor.instances,2)\n",
      "(spark.history.fs.logdirectory,hdfs://oonisim:8020/logs_spark)\n",
      "(spark.default.parallelism,6)\n",
      "(spark.executor.id,driver)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.master,yarn)\n",
      "(spark.ui.filters,org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter)\n",
      "(spark.executor.memory,5g)\n",
      "(spark.eventLog.dir,hdfs://oonisim:8020/logs_spark)\n",
      "(spark.executor.cores,4)\n",
      "(spark.driver.appUIAddress,http://10.186.87.0:4040)\n",
      "(spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS,oonisim)\n",
      "(spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES,http://oonisim:8088/proxy/application_1576122573478_0006)\n",
      "(spark.app.id,application_1576122573478_0006)\n",
      "(spark.sql.shuffle.partitions,6)\n"
     ]
    }
   ],
   "source": [
    "configMap.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CSV_DELIMITER = ,\n",
       "PROTOCOL = file://\n",
       "DATA_DIR = /home/oonisim/home/repositories/git/oonisim/spark-programs/Flight/src/main/jupyter/\n",
       "FLIGHTDATA_CSV_PATH = file:///home/oonisim/home/repositories/git/oonisim/spark-programs/Flight/src/main/jupyter/../resources/flightData.csv\n",
       "PASSENGER_CSV_PATH = file:///home/oonisim/home/repositories/git/oonisim/spark-programs/Flight/src/main/jupyter/../resources/passengers.csv\n",
       "DATE_FORMAT = yyyy-MM-dd\n",
       "FLIGHT_DATE_FROM = 2017-01-01\n",
       "FLIGHT_DATE_TO = 2017-12-31\n",
       "NUM_FLIGHT_TOGETHER = 3\n",
       "RESULT_DIR = results/flightsTogetherMatrix\n",
       "DEBUG = false\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val CSV_DELIMITER = \",\"\n",
    "//val FLIGHTDATA_CSV_PATH = \"test.csv\"  // First teset with this as in the snapshot.\n",
    "\n",
    "val PROTOCOL=\"file://\"\n",
    "val DATA_DIR=\"/home/oonisim/home/repositories/git/oonisim/spark-programs/Flight/src/main/jupyter/\"\n",
    "val FLIGHTDATA_CSV_PATH = PROTOCOL + DATA_DIR + \"../resources/flightData.csv\"\n",
    "val PASSENGER_CSV_PATH = PROTOCOL + DATA_DIR + \"../resources/passengers.csv\"\n",
    "\n",
    "val DATE_FORMAT = \"yyyy-MM-dd\"\n",
    "val FLIGHT_DATE_FROM = \"2017-01-01\"\n",
    "val FLIGHT_DATE_TO   = \"2017-12-31\"\n",
    "val NUM_FLIGHT_TOGETHER = 3\n",
    "\n",
    "val RESULT_DIR = \"results/flightsTogetherMatrix\"\n",
    "\n",
    "val DEBUG = false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timing = \n",
       "times = ListBuffer()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "clear: ()Unit\n",
       "average: ()Long\n",
       "timed: [T](label: String, code: => T)T\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ListBuffer()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ListBuffer\n",
    "\n",
    "val timing = new StringBuffer\n",
    "val times = new ListBuffer[Long]()\n",
    "\n",
    "def clear(): Unit = {\n",
    "    timing.setLength(0)\n",
    "    times.clear\n",
    "}\n",
    "def average(): Long = {\n",
    "    times.reduce(_+_) / times.length\n",
    "}\n",
    "\n",
    "/**\n",
    "@param label Description about the run\n",
    "@code code to execute\n",
    "@return execution\n",
    "*/\n",
    "def timed[T](label: String, code: => T): T = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val result = code\n",
    "    val stop = System.currentTimeMillis()\n",
    "    timing.append(s\"Processing $label took ${stop - start} ms.\\n\")\n",
    "    times.append(stop - start)\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:53: error: missing argument list for method timed\n",
       "Unapplied methods are only converted to functions when a function type is expected.\n",
       "You can make this conversion explicit by writing `timed _` or `timed(_,_)` instead of `timed`.\n",
       "       timed\n",
       "       ^\n",
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// To flush out error: missing argument list for method timed\n",
    "println(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "save: (df: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save(df: DataFrame) = {\n",
    "    df.coalesce(1)\n",
    "    .write\n",
    "    .format(\"csv\")\n",
    "    .mode(SaveMode.Overwrite)\n",
    "    .option(\"header\", \"true\")\n",
    "    .save(RESULT_DIR)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame\\[passengerId, flightId\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema = StructType(StructField(passengerId,IntegerType,true), StructField(flightId,IntegerType,true), StructField(from,StringType,true), StructField(to,StringType,true), StructField(date,TimestampType,true))\n",
       "flightData = [passengerId: int, flightId: int]\n",
       "maxPassengerId = 15499\n",
       "maxFlightId = 999\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transformations, no action yet\n",
    "val schema = StructType(Array(\n",
    "    StructField(\"passengerId\",IntegerType,true), \n",
    "    StructField(\"flightId\" ,IntegerType,true), \n",
    "    StructField(\"from\",StringType,true), \n",
    "    StructField(\"to\",StringType,true), \n",
    "    StructField(\"date\",TimestampType,true)\n",
    "))  \n",
    "\n",
    "val flightData = spark.read.format(\"csv\")\n",
    "    .schema(schema)\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    //.option(\"inferSchema\", \"true\")\n",
    "    .load(FLIGHTDATA_CSV_PATH)\n",
    "    .selectExpr(\n",
    "        // NOTE_PASSENGER_ID_ADJUST: To start passengerId from 0\n",
    "        \"passengerId - 1 AS passengerId\", \n",
    "        \"flightId\"\n",
    "    )\n",
    "    /*\n",
    "    .orderBy(\n",
    "        asc(\"passengerId\"), \n",
    "        asc(\"flightId\")\n",
    "    )\n",
    "    */\n",
    "\n",
    "val maxPassengerId = flightData\n",
    "    .select(max(\"passengerId\"))\n",
    "    .first\n",
    "    .getInt(0)\n",
    "\n",
    "val maxFlightId = flightData\n",
    "    .select(max(\"flightId\"))\n",
    "    .first\n",
    "    .getInt(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passenger to flight counters as RDD\\[IndexedRow\\]\n",
    "Convert each Dataframe row (passengerId, flightId) into (passegnerId, (flightId, 1)+) pair as an IndexedRow.  \n",
    "e.g. for passenger 1 who had flights \\[1, 16, 234\\]:\n",
    "\n",
    "<pre>\n",
    "[ (1, (1,  1.0)   // passengerId 1 had flightId 1  for 1.0 times.  \n",
    "  (1, (16, 1.0)   // passegnerId 1 had flightId 16 for 1.0 times\n",
    "  (1, (234,1.0) ]  \n",
    "</pre>\n",
    "\n",
    "as\n",
    "<pre>\n",
    "RDD[IndexedRow(  \n",
    "  passengerId,  \n",
    "  Vectors.sparse(\n",
    "      totalFlights,  # To accommodate the max number of flights (0, 1, ... totalFlights -1)\n",
    "      Iterable(flightId, 1.0)    \n",
    "  )\n",
    "]\n",
    "</pre>\n",
    "\n",
    "In reality, flight ID will not be continous numbers, but to allocate minimum storage, allocate the number from 0 to totalFlights -1. Same to the passenger ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toIndexedRow: (passengerToFlightsMap: (Int, Iterable[Int]), maxFlightId: Int)org.apache.spark.mllib.linalg.distributed.IndexedRow\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    "Create a IndexedRow that maps a passengerId to a sparse vector of (flightId, 1)+\n",
    "from (passengerId, Iterable[flightId]).\n",
    "\n",
    "The dimension of the vector in IndexedRow is the number of total flights so that\n",
    "we can mark each flight per passenger in the (passengerId/row, flightId/column) matrix\n",
    "\n",
    "@param passengerToFlightsKV (passengerId, Iterable(flightId)) key value pair\n",
    "@param maxFlightId maximum flightId\n",
    "@Return \n",
    "    IndexedRow(  \n",
    "      passengerId,  \n",
    "      Vectors.sparse(\n",
    "          totalFlights,  \n",
    "          Iterable(flightId, 1.0)    \n",
    "      )\n",
    "    ]\n",
    "*/\n",
    "def toIndexedRow(passengerToFlightsMap:(Int, Iterable[Int]), maxFlightId: Int): IndexedRow = {\n",
    "    passengerToFlightsMap match {\n",
    "        case (passengerId, flightIDs) => {\n",
    "            //--------------------------------------------------------------------------------\n",
    "            // flightIDs to Seq((flightId, 1)) eg\n",
    "            // (1, 16, 234) => \n",
    "            // [ (1,   1),   <-- (flightId 1,   count 1)\n",
    "            //   (16,  1),   <-- (flightId 16,  count 1)\n",
    "            //   (234, 1) ]  <-- (flightId 234, count 1)\n",
    "            //--------------------------------------------------------------------------------\n",
    "            val flightCountKV = flightIDs.map(i => (i, 1.0)).toSeq\n",
    "\n",
    "            //--------------------------------------------------------------------------------\n",
    "            // IndexedRow(passengerId, Vector((flightID, 1)))\n",
    "            // When passengerId 1 took flight (1, 16, 234) then\n",
    "            // IndexedRow(1, ((1, 1), (16, 1), (234, 1))\n",
    "            //\n",
    "            // Allocate storage to accommodate maxFlightId + 1.\n",
    "            //--------------------------------------------------------------------------------\n",
    "            new IndexedRow (\n",
    "                passengerId,\n",
    "                Vectors.sparse(maxFlightId + 1, flightCountKV)\n",
    "            )\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "/*val row = toIndexedRow(\n",
    "    (1, Iterable(29, 75, 101, 131, 189, 217, 247, 320, 352, 362, 717, 736, 740)),\n",
    "    totalFlights.toInt\n",
    ")\n",
    "row\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "passengerToFlightCounters = MapPartitionsRDD[20] at map at <console>:61\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val passengerToFlightCounters= flightData.rdd\n",
    "    .map(rowPF => (rowPF.getInt(0), rowPF.getInt(1)))  // Out from ROW[(passengerId, flightId)]\n",
    "    .groupByKey()                                      // (passengerId, Iterable(flightId))\n",
    "//    .sortBy(row => row._1)                           // sort asc(passengerId)\n",
    "    .map(\n",
    "        passengerToFlightIDsMap => toIndexedRow(passengerToFlightIDsMap, maxFlightId)\n",
    "    )                                            // IndexedRow(passengerId, Vector((flightId, 1)))\n",
    "\n",
    "if (DEBUG){\n",
    "    passengerToFlightCounters.take(3).foreach(println)\n",
    "    passengerToFlightCounters.count()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix M of (passengerId/row, flightId/col)\n",
    "\n",
    "* IndexedRowMatrix [multiply](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@multiply(B:org.apache.spark.mllib.linalg.Matrix):org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix) method requires *a local matrix* on the right.\n",
    "* [BlockMatrix](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.BlockMatrix) is the only Spark matrix that supports multiply in distributed manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "passengerFlightIndexedMatrix = org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@46022457\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@46022457"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//--------------------------------------------------------------------------------\n",
    "// Create IndexedRowMatrix of (passengerId as row, flightId as column) with binary \n",
    "// 0.0 or 1.0 as its value to mark if the passenger took the flightId.\n",
    "// The passengerId is the index to sort by asc(passengerId).\n",
    "// \n",
    "// IndexedRowMatrix multiply method requires a local matrix on the right.\n",
    "// BlockMatrix is the only Spark matrix that supports multiply in distributed manner.\n",
    "//--------------------------------------------------------------------------------\n",
    "// IndexedRow matrix\n",
    "val passengerFlightIndexedMatrix = new IndexedRowMatrix(passengerToFlightCounters)\n",
    "\n",
    "if (DEBUG){\n",
    "    val rowNums = passengerFlightIndexedMatrix.numRows\n",
    "    val colNums = passengerFlightIndexedMatrix.numCols\n",
    "    println(s\"($rowNums, $colNums)\")\n",
    "    println(passengerFlightIndexedMatrix.getClass)\n",
    "\n",
    "    passengerFlightIndexedMatrix\n",
    "        .rows                    // RDD[IndexedRow] where IndexedRow is (index, vector)\n",
    "//        .sortBy(iv => iv.index)  // Sort by IndexedRow.index\n",
    "        .cache\n",
    "        .toDF\n",
    "        .show(true)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "passengerFlightBlockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@3930bb6d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.BlockMatrix@3930bb6d"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// BlockMatrix\n",
    "val passengerFlightBlockMatrix = passengerFlightIndexedMatrix.toBlockMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose M<sup>T</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "passengerFlightBlockMatrixTransposed = org.apache.spark.mllib.linalg.distributed.BlockMatrix@6e983d1d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.BlockMatrix@6e983d1d"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val passengerFlightBlockMatrixTransposed = passengerFlightBlockMatrix\n",
    "    .transpose\n",
    "\n",
    "if (DEBUG){\n",
    "    println(passengerFlightBlockMatrixTransposed.getClass)\n",
    "    println(\n",
    "        passengerFlightBlockMatrixTransposed.numRows, \n",
    "        passengerFlightBlockMatrixTransposed.numCols\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product of M * M<sup>T</sup> as Flown Together Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightsTogetherIndexedMatrix = org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@4b7ed451\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@4b7ed451"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightsTogetherIndexedMatrix = passengerFlightBlockMatrix\n",
    "    .multiply(passengerFlightBlockMatrixTransposed)\n",
    "    .toIndexedRowMatrix\n",
    "\n",
    "if (DEBUG){\n",
    "    val numRows = flightsTogetherIndexedMatrix.numRows\n",
    "    val numCols = flightsTogetherIndexedMatrix.numCols\n",
    "    println(flightsTogetherIndexedMatrix.getClass)\n",
    "    println(s\"shape is $numRows, $numCols\")\n",
    "    \n",
    "    //--------------------------------------------------------------------------------\n",
    "    // View the result in DataFrame\n",
    "    //--------------------------------------------------------------------------------\n",
    "    flightsTogetherIndexedMatrix\n",
    "        .rows                    // RDD[IndexedRow] where IndexedRow is (index, vector)\n",
    "        //.sortBy(iv => iv.index)  // Sort by IndexedRow.index\n",
    "        .toDF\n",
    "        .selectExpr(\n",
    "            \"index AS passengerId\",\n",
    "            \"vector AS companionIDs\"\n",
    "        )\n",
    "        .show(true)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD(passengerId, companionId, numFlightsShared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flownTogether: (passengerFlightsTuple: (Int, Array[Double]), threshold: Int)scala.collection.immutable.IndexedSeq[(Int, Int, Double)]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    "    row(passengerId, numsFlightsTogether) is each row of the M*M.T product.\n",
    "    \n",
    "    numsFlightsTogether is a list of numbers each of which tells the number of flights\n",
    "    shared with another passenger.\n",
    "    \n",
    "    numsFlightsTogether(0) For a passengerId 0\n",
    "    --------------------------------------------------------------------------------\n",
    "    (index) |0  |1|2|3|4|5|6|\n",
    "    (number)|100|3|0|1|2|0|0|  <-- Total flights of passengerId 0 is 100.\n",
    "                                   Shared 3 flights with passengerId 1.\n",
    "                                   Shared 1 flights with passengerId 2 ...\n",
    "\n",
    "    numsFlightsTogether(1) For a passengerId 1\n",
    "    --------------------------------------------------------------------------------\n",
    "    (index) |0|1  |2|3|4|5|6|\n",
    "    (number)|3|100|7|0|2|0|0|  <-- Total flights of passengerId 1 is 100.\n",
    "                                   Shared 3 flights with passengerId 0\n",
    "                                   Shared 7 flights with passengerId 2 ...\n",
    "\n",
    "    numsFlightsTogether(2) For a passengerId 2\n",
    "    --------------------------------------------------------------------------------\n",
    "    (index) |0|1|2  |3|4|5|6|\n",
    "    (number)|0|7|100|0|2|0|0|  <-- Total flights of passengerId 2 is 100.\n",
    "                                   Shared 0 flights with passengerId 0\n",
    "                                   Shared 7 flights with passengerId 1 ...\n",
    "\n",
    "@param row (passengerId, numsFlightsTogether)\n",
    "@return Seq(passengerId, companionId, numberOfFlightsShared)\n",
    " */\n",
    "def flownTogether(passengerFlightsTuple: (Int, Array[Double]), threshold:Int = NUM_FLIGHT_TOGETHER) = {\n",
    "    val _index = passengerFlightsTuple._1\n",
    "    val _array = passengerFlightsTuple._2\n",
    "    val passengerId = _index          // passengerId itself is the row index\n",
    "    val numsFlightsTogether = _array  // number of flights shared another\n",
    "    \n",
    "    for{\n",
    "        fellowPassengerId <- (passengerId + 1 to maxPassengerId)\n",
    "        if(numsFlightsTogether(fellowPassengerId) > threshold)\n",
    "    } yield (\n",
    "        //-------------------------------------------------------------------------------\n",
    "        // NOTE_PASSENGER_ID_ADJUST: Subtracted -1 ao start passengerId from 0\n",
    "        // Hence, revert to original as it starts at 1, not 0 \n",
    "        //-------------------------------------------------------------------------------\n",
    "        passengerId + 1,               \n",
    "        fellowPassengerId + 1, \n",
    "        numsFlightsTogether(fellowPassengerId)\n",
    "    )\n",
    "}\n",
    "\n",
    "/* Check first\n",
    "val first = flightTogetherMatrix.rows.first\n",
    "flownTogether(first.index.toInt, first.vector.toArray)\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightTogetherRDD = MapPartitionsRDD[40] at flatMap at <console>:61\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[40] at flatMap at <console>:61"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightTogetherRDD = flightsTogetherIndexedMatrix\n",
    "    .rows\n",
    "    //--------------------------------------------------------------------------------\n",
    "    // Convert row of product M*M.T into (passengerId, compaionId, numFlightsShared).\n",
    "    //--------------------------------------------------------------------------------\n",
    "    .flatMap(\n",
    "        indexedRow => flownTogether(\n",
    "            (indexedRow.index.toInt, indexedRow.vector.toArray),\n",
    "            NUM_FLIGHT_TOGETHER\n",
    "        )\n",
    "    )\n",
    "    //--------------------------------------------------------------------------------\n",
    "    // Sort (passengerId, companionId, numFlightsShared)._1\n",
    "    //--------------------------------------------------------------------------------\n",
    "    //.sortBy(pcn => pcn._3, false)\n",
    "    //.saveAsText(RESULT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightTogetherDF = [Passenger 1 ID: int, Passenger 2 ID: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Passenger 1 ID: int, Passenger 2 ID: int ... 1 more field]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightTogetherDF = flightTogetherRDD\n",
    "    .toDF\n",
    "    .selectExpr(\n",
    "        \"_1 AS `Passenger 1 ID`\",\n",
    "        \"_2 AS `Passenger 2 ID`\",\n",
    "        \"CAST(_3 AS Int) AS `Number of flights together`\"        \n",
    "    )\n",
    "    .sort(\n",
    "        desc(\"Number of flights together\"),\n",
    "        asc(\"Passenger 1 ID\"),\n",
    "        asc(\"Passenger 2 ID\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "timed(\n",
    "    \"Flight togethe matrix\",\n",
    "    save(flightTogetherDF)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 3 flights shared and the passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Flight togethe matrix took 51078 ms.\n",
      "\n",
      "+--------------+--------------+--------------------------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|\n",
      "+--------------+--------------+--------------------------+\n",
      "|           701|           760|                        15|\n",
      "|          2717|          2759|                        14|\n",
      "|          3503|          3590|                        14|\n",
      "+--------------+--------------+--------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(timing)\n",
    "flightTogetherDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "[flightsTogetherMatrix.csv](results/flightsTogetherMatrix.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance\n",
    "\n",
    "Spark SQL took 3 sec, whereas matrix took 51 secs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

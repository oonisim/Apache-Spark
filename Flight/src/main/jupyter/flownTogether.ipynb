{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 04 Take 2\n",
    "\n",
    "This implementation is somehow extremely slow. Need to investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import java.time.temporal.ChronoUnit\n",
    "import java.time.{Period, LocalDate, Instant}\n",
    "import java.sql.Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark parition control based on core availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NUM_CORES = 4\n",
       "NUM_PARTITIONS = 3\n",
       "spark = <lazy>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<lazy>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NUM_CORES = 4\n",
    "val NUM_PARTITIONS = 3\n",
    "\n",
    "lazy val spark: SparkSession = SparkSession.builder()\n",
    "    .master(\"local\")\n",
    "    .appName(\"flight\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CSV_DELIMITER = ,\n",
       "FLIGHTDATA_CSV_PATH = ../resources/flightData.csv\n",
       "PASSENGER_CSV_PATH = ../resources/passengers.csv\n",
       "DATE_FORMAT = yyyy-MM-dd\n",
       "FLIGHT_DATE_FROM = 2017-01-01\n",
       "FLIGHT_DATE_TO = 2017-12-31\n",
       "NUM_FLIGHT_TOGETHER = 3\n",
       "RESULT_DIR = results/flightsTogether\n",
       "DEBUG = true\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val CSV_DELIMITER = \",\"\n",
    "val FLIGHTDATA_CSV_PATH = \"../resources/flightData.csv\"\n",
    "val PASSENGER_CSV_PATH = \"../resources/passengers.csv\"\n",
    "\n",
    "val DATE_FORMAT = \"yyyy-MM-dd\"\n",
    "val FLIGHT_DATE_FROM = \"2017-01-01\"\n",
    "val FLIGHT_DATE_TO   = \"2017-12-31\"\n",
    "val NUM_FLIGHT_TOGETHER = 3\n",
    "\n",
    "val RESULT_DIR = \"results/flightsTogether\"\n",
    "\n",
    "val DEBUG = true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timing = \n",
       "times = ListBuffer()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "clear: ()Unit\n",
       "average: ()Long\n",
       "timed: [T](label: String, code: => T)T\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ListBuffer()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ListBuffer\n",
    "\n",
    "val timing = new StringBuffer\n",
    "val times = new ListBuffer[Long]()\n",
    "\n",
    "def clear(): Unit = {\n",
    "    timing.setLength(0)\n",
    "    times.clear\n",
    "}\n",
    "def average(): Long = {\n",
    "    times.reduce(_+_) / times.length\n",
    "}\n",
    "\n",
    "/**\n",
    "@param label Description about the run\n",
    "@code code to execute\n",
    "@return execution\n",
    "*/\n",
    "def timed[T](label: String, code: => T): T = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val result = code\n",
    "    val stop = System.currentTimeMillis()\n",
    "    timing.append(s\"Processing $label took ${stop - start} ms.\\n\")\n",
    "    times.append(stop - start)\n",
    "    code\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:45: error: missing argument list for method timed\n",
       "Unapplied methods are only converted to functions when a function type is expected.\n",
       "You can make this conversion explicit by writing `timed _` or `timed(_,_)` instead of `timed`.\n",
       "       timed\n",
       "       ^\n",
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// To flush out error: missing argument list for method timed\n",
    "println(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "save: (df: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save(df: DataFrame) = {\n",
    "    df.coalesce(1)\n",
    "    .write\n",
    "    .format(\"csv\")\n",
    "    .mode(SaveMode.Overwrite)\n",
    "    .option(\"header\", \"true\")\n",
    "    .save(RESULT_DIR)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base FlightData DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightData = [passengerId: int, flightId: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 1 more field]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transformations, no action yet\n",
    "val flightData = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(FLIGHTDATA_CSV_PATH)\n",
    "    .select(\n",
    "        \"passengerId\",\n",
    "        \"flightId\",\n",
    "        \"date\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "queryFlightsTogether: (atLeastNTimes: Int, fromDate: String, toDate: String)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def queryFlightsTogether(atLeastNTimes: Int, fromDate: String, toDate: String): String = {\n",
    "    val query = s\"\"\"\n",
    "    WITH \n",
    "        --------------------------------------------------------------------------------\n",
    "        -- Passengers flew more than atLeastNTimes times.\n",
    "        --------------------------------------------------------------------------------\n",
    "        more_than_n_flights AS (\n",
    "            SELECT passengerId\n",
    "            FROM flightData\n",
    "            GROUP BY passengerId\n",
    "            HAVING count(flightId) > $atLeastNTimes\n",
    "            ORDER BY passengerId\n",
    "        )\n",
    "\n",
    "    SELECT \n",
    "        f.passengerId AS `Passenger 1 ID`, \n",
    "        s.passengerId AS `Passenger 2 ID`, \n",
    "        count(s.flightId) AS `Number of flights together`,\n",
    "        '$fromDate' as From,\n",
    "        '$toDate' as To\n",
    "    FROM\n",
    "        flightData f \n",
    "        --------------------------------------------------------------------------------\n",
    "        -- Passengers more than atLeastNTimes flights\n",
    "        --------------------------------------------------------------------------------\n",
    "        INNER JOIN more_than_n_flights m \n",
    "            ON f.passengerId == m.passengerId\n",
    "        --------------------------------------------------------------------------------\n",
    "        -- Passengers who shared same flights\n",
    "        --------------------------------------------------------------------------------\n",
    "        INNER JOIN flightData s \n",
    "            ON f.flightId == s.flightId\n",
    "    WHERE\n",
    "        f.passengerId < s.passengerId AND\n",
    "        f.date >= to_timestamp('$fromDate', '$DATE_FORMAT') AND\n",
    "        f.date <= to_timestamp('$toDate',   '$DATE_FORMAT') AND\n",
    "        s.date >= to_timestamp('$fromDate', '$DATE_FORMAT') AND\n",
    "        s.date <= to_timestamp('$toDate',   '$DATE_FORMAT')\n",
    "    GROUP BY \n",
    "        f.passengerId, s.passengerId\n",
    "    HAVING \n",
    "        count(s.flightId) > $atLeastNTimes\n",
    "    --ORDER BY \n",
    "    --    f.passengerId, s.passengerId\n",
    "    \"\"\"\n",
    "    query\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flownTogether: (atLeastNTimes: Int, from: String, to: String)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    "Get the dataframe of |passenger 1|passenger 2|num flights| from | to|\n",
    "@param atLeastNTimes Number beyond which passengers share flights\n",
    "@param from Date including and after which the shared flight was scheduled\n",
    "@param to Data including and before which the shared flight was scheduled\n",
    "@return DataFrame\n",
    "*/\n",
    "def flownTogether(atLeastNTimes: Int, from: String, to: String) = {\n",
    "    val execution = spark\n",
    "        .sql(queryFlightsTogether(atLeastNTimes, from, to))\n",
    "        .sort(\n",
    "            desc(\"Number of flights together\"), \n",
    "            asc(\"Passenger 1 ID\"),\n",
    "            asc(\"Passenger 2 ID\")\n",
    "        )\n",
    "    execution\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility to run multiple times and time the execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run: (label: String, repeats: Int)Long\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    "Run the SparkSQL \n",
    "@param label Lable to describe this run\n",
    "@param query SQL \n",
    "@param repeats Number of run\n",
    "@return Average execution time in msec\n",
    "*/\n",
    "def run(label: String, repeats: Int): Long = {\n",
    "    flightData\n",
    "        .persist\n",
    "        .createOrReplaceTempView(\"flightData\")\n",
    "\n",
    "    val execution = flownTogether(\n",
    "        NUM_FLIGHT_TOGETHER,\n",
    "        FLIGHT_DATE_FROM,\n",
    "        FLIGHT_DATE_TO\n",
    "    )\n",
    "    clear()\n",
    "    for (i <- (0 until repeats)){\n",
    "        timed(\n",
    "            label,\n",
    "            execution.show(5)\n",
    "        )\n",
    "        println(timing)\n",
    "        println(s\"Average time $average ms\")\n",
    "    }\n",
    "    println(execution.rdd.toDebugString)    \n",
    "    save(execution)\n",
    "\n",
    "    if (DEBUG) println(execution.rdd.toDebugString)    \n",
    "\n",
    "    spark.catalog.dropTempView(\"flightData\")\n",
    "    flightData.unpersist\n",
    "    \n",
    "    average\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution 01\n",
    "Order by (passengerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|           701|           760|                        15|2017-01-01|2017-12-31|\n",
      "|           760|           701|                        15|2017-01-01|2017-12-31|\n",
      "|          2717|          2759|                        14|2017-01-01|2017-12-31|\n",
      "|          2759|          2717|                        14|2017-01-01|2017-12-31|\n",
      "|          3503|          3590|                        14|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|           701|           760|                        15|2017-01-01|2017-12-31|\n",
      "|           760|           701|                        15|2017-01-01|2017-12-31|\n",
      "|          2717|          2759|                        14|2017-01-01|2017-12-31|\n",
      "|          2759|          2717|                        14|2017-01-01|2017-12-31|\n",
      "|          3503|          3590|                        14|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by (passengerId) took 11855 ms.\n",
      "\n",
      "Average time 11855 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|           701|           760|                        15|2017-01-01|2017-12-31|\n",
      "|           760|           701|                        15|2017-01-01|2017-12-31|\n",
      "|          2717|          2759|                        14|2017-01-01|2017-12-31|\n",
      "|          2759|          2717|                        14|2017-01-01|2017-12-31|\n",
      "|          3503|          3590|                        14|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|           701|           760|                        15|2017-01-01|2017-12-31|\n",
      "|           760|           701|                        15|2017-01-01|2017-12-31|\n",
      "|          2717|          2759|                        14|2017-01-01|2017-12-31|\n",
      "|          2759|          2717|                        14|2017-01-01|2017-12-31|\n",
      "|          3503|          3590|                        14|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by (passengerId) took 11855 ms.\n",
      "Processing Order by (passengerId) took 8975 ms.\n",
      "\n",
      "Average time 10415 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|           701|           760|                        15|2017-01-01|2017-12-31|\n",
      "|           760|           701|                        15|2017-01-01|2017-12-31|\n",
      "|          2717|          2759|                        14|2017-01-01|2017-12-31|\n",
      "|          2759|          2717|                        14|2017-01-01|2017-12-31|\n",
      "|          3503|          3590|                        14|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|           701|           760|                        15|2017-01-01|2017-12-31|\n",
      "|           760|           701|                        15|2017-01-01|2017-12-31|\n",
      "|          2717|          2759|                        14|2017-01-01|2017-12-31|\n",
      "|          2759|          2717|                        14|2017-01-01|2017-12-31|\n",
      "|          3503|          3590|                        14|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by (passengerId) took 11855 ms.\n",
      "Processing Order by (passengerId) took 8975 ms.\n",
      "Processing Order by (passengerId) took 9105 ms.\n",
      "\n",
      "Average time 9978 ms\n",
      "(12) MapPartitionsRDD[213] at rdd at <console>:79 []\n",
      " |   MapPartitionsRDD[212] at rdd at <console>:79 []\n",
      " |   MapPartitionsRDD[211] at rdd at <console>:79 []\n",
      " |   ShuffledRowRDD[210] at rdd at <console>:79 []\n",
      " +-(12) MapPartitionsRDD[209] at rdd at <console>:79 []\n",
      "    |   MapPartitionsRDD[205] at rdd at <console>:79 []\n",
      "    |   ShuffledRowRDD[204] at rdd at <console>:79 []\n",
      "    +-(1) MapPartitionsRDD[203] at rdd at <console>:79 []\n",
      "       |  MapPartitionsRDD[202] at rdd at <console>:79 []\n",
      "       |  MapPartitionsRDD[201] at rdd at <console>:79 []\n",
      "       |  MapPartitionsRDD[200] at rdd at <console>:79 []\n",
      "       |  *(1) FileScan csv [passengerId#10,flightId#11,date#14] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/masa/home/repository/git/oonisim/spark-programs/Flight/src/main/reso..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int,flightId:int,date:timestamp>\n",
      " MapPartitionsRDD[11] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |      CachedPartitions: 1; MemorySize: 389.6 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n",
      "       |  MapPartitionsRDD[10] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |  FileScanRDD[9] at run at ThreadPoolExecutor.java:1149 []\n",
      "(12) MapPartitionsRDD[213] at rdd at <console>:79 []\n",
      " |   MapPartitionsRDD[212] at rdd at <console>:79 []\n",
      " |   MapPartitionsRDD[211] at rdd at <console>:79 []\n",
      " |   ShuffledRowRDD[210] at rdd at <console>:79 []\n",
      " +-(12) MapPartitionsRDD[209] at rdd at <console>:79 []\n",
      "    |   MapPartitionsRDD[205] at rdd at <console>:79 []\n",
      "    |   ShuffledRowRDD[204] at rdd at <console>:79 []\n",
      "    +-(1) MapPartitionsRDD[203] at rdd at <console>:79 []\n",
      "       |  MapPartitionsRDD[202] at rdd at <console>:79 []\n",
      "       |  MapPartitionsRDD[201] at rdd at <console>:79 []\n",
      "       |  MapPartitionsRDD[200] at rdd at <console>:79 []\n",
      "       |  *(1) FileScan csv [passengerId#10,flightId#11,date#14] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/masa/home/repository/git/oonisim/spark-programs/Flight/src/main/reso..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int,flightId:int,date:timestamp>\n",
      " MapPartitionsRDD[11] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |      CachedPartitions: 1; MemorySize: 389.6 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n",
      "       |  MapPartitionsRDD[10] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |  FileScanRDD[9] at run at ThreadPoolExecutor.java:1149 []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [passengerId: int, flightId: int ... 1 more field]\n",
       "timeOrderPassenger = 9978\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 1 more field]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = flightData\n",
    "    .orderBy(\"passengerId\")\n",
    "\n",
    "val timeOrderPassenger = run(\n",
    "    \"Order by (passengerId)\",\n",
    "    3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

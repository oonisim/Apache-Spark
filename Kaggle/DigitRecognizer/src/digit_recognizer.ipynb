{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer Training\n",
    "Trainig part of [Kaggle Digit Reconizer](https://www.kaggle.com/c/digit-recognizer/overview).\n",
    "\n",
    "1. [Compute SVD with Spark Row Matrix](https://spark.apache.org/docs/2.2.0/mllib-dimensionality-reduction.html#singular-value-decomposition-svd) to decide the number of principal components (k) to use for PCA.\n",
    "2. [PCA on Spark](https://spark.apache.org/docs/2.2.0/mllib-dimensionality-reduction.html#principal-component-analysis-pca) to reduce dimensions to k.\n",
    "3. [Random Forest Classifer on Spark](https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier) to train a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification._\n",
    "\n",
    "import org.apache.spark.ml.linalg._\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.linalg.distributed._\n",
    "\n",
    "\n",
    "import java.time.{Period, LocalDate, Instant}\n",
    "import java.sql.Timestamp\n",
    "import java.time.temporal.ChronoUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timing = \n",
       "times = ListBuffer()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "clear: ()Unit\n",
       "average: ()Long\n",
       "timed: [T](label: String, code: => T)T\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ListBuffer()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ListBuffer\n",
    "\n",
    "val timing = new StringBuffer\n",
    "val times = new ListBuffer[Long]()\n",
    "\n",
    "def clear(): Unit = {\n",
    "    timing.setLength(0)\n",
    "    times.clear\n",
    "}\n",
    "def average(): Long = {\n",
    "    times.reduce(_+_) / times.length\n",
    "}\n",
    "\n",
    "/**\n",
    "@param label Description about the run\n",
    "@code code to execute\n",
    "@return execution\n",
    "*/\n",
    "def timed[T](label: String, code: => T): T = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val result = code\n",
    "    val stop = System.currentTimeMillis()\n",
    "    timing.append(s\"Processing $label took ${stop - start} ms.\\n\")\n",
    "    times.append(stop - start)\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:60: error: missing argument list for method timed\n",
       "Unapplied methods are only converted to functions when a function type is expected.\n",
       "You can make this conversion explicit by writing `timed _` or `timed(_,_)` instead of `timed`.\n",
       "       timed\n",
       "       ^\n",
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!-- To left align the HTML components in Markdown -->\n",
       "<style>\n",
       "table {float:left}\n",
       "</style>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html\n",
    "<!-- To left align the HTML components in Markdown -->\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "NUM_CORES = 8\n",
       "NUM_PARTITIONS = 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NUM_CORES = 4\n",
    "val NUM_PARTITIONS = 4\n",
    "\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "/*\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.driver.memory\", \"6g\")\n",
    "spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "spark.conf.set(\"spark.master\", \"spark://masa:7077\")\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.serializer,org.apache.spark.serializer.KryoSerializer)\n",
      "(spark.driver.host,10.115.130.67)\n",
      "(spark.eventLog.enabled,true)\n",
      "(spark.driver.port,32793)\n",
      "(spark.hadoop.validateOutputSpecs,True)\n",
      "(spark.repl.class.uri,spark://10.115.130.67:32793/classes)\n",
      "(spark.jars,file:/home/oonisim/.local/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.3.0-incubating.jar)\n",
      "(spark.repl.class.outputDir,/tmp/spark-854365bf-9f7d-4a6b-bbb4-76af2232bcb3/repl-335596a3-b186-46d8-ab2f-66c1945f4ad8)\n",
      "(spark.app.name,Apache Toree)\n",
      "(spark.driver.memory,1g)\n",
      "(spark.executor.instances,3)\n",
      "(spark.history.fs.logdirectory,hdfs://oonisim:8020/logs_spark)\n",
      "(spark.default.parallelism,32)\n",
      "(spark.executor.id,driver)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.master,yarn)\n",
      "(spark.ui.filters,org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter)\n",
      "(spark.executor.memory,4g)\n",
      "(spark.eventLog.dir,hdfs://oonisim:8020/logs_spark)\n",
      "(spark.executor.cores,4)\n",
      "(spark.driver.appUIAddress,http://10.115.130.67:4040)\n",
      "(spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS,oonisim)\n",
      "(spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES,http://oonisim:8088/proxy/application_1577335006010_0003)\n",
      "(spark.app.id,application_1577335006010_0003)\n",
      "(spark.sql.shuffle.partitions,32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "configMap: Unit = ()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val configMap = spark.conf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROTOCOL = hdfs://\n",
       "DATA_DIR = /data/kaggle/mnist\n",
       "MODEL_DIR = /data/model\n",
       "RESULT_DIR = .\n",
       "COVERAGE = 0.95\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val PROTOCOL=\"hdfs://\"\n",
    "val DATA_DIR=\"/data/kaggle/mnist/data\"\n",
    "val MODEL_DIR=\"/data/kaggle/mnist/model\"\n",
    "val RESULT_DIR=\"/data/kaggle/mnist/result\"\n",
    "\n",
    "val CROSS_VALIDATE_NUM_HOLDS = 5\n",
    "val CROSS_VALIDATE_PARALLELISM = NUM_CORES\n",
    "\n",
    "val COVERAGE:Double = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train = [label: int, pixel0: int ... 783 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: int, pixel0: int ... 783 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val train = spark.read.format(\"csv\")\n",
    "    .option(\"header\", true)\n",
    "    .option(\"inferSchema\", true)\n",
    "    .load(PROTOCOL + DATA_DIR + \"/train.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numFeatures = 784\n",
       "numSamples = 42000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numFeatures = train.columns.size - 1  // Remove \"label\" column\n",
    "val numSamples = train.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe for ML training\n",
    "\n",
    "Use R formula transformation to create label and features columns for Spark ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing RFormula took 811 ms.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "formula = label ~ .\n",
       "rformula = RFormulaModel(ResolvedRFormula(label=label, terms=[pixel0,pixel1,pixel2,pixel3,pixel4,pixel5,pixel6,pixel7,pixel8,pixel9,pixel10,pixel11,pixel12,pixel13,pixel14,pixel15,pixel16,pixel17,pixel18,pixel19,pixel20,pixel21,pixel22,pixel23,pixel24,pixel25,pixel26,pixel27,pixel28,pixel29,pixel30,pixel31,pixel32,pixel33,pixel34,pixel35,pixel36,pixel37,pixel38,pixel39,pixel40,pixel41,pixel42,pixel43,pixel44,pixel45,pixel46,pixel47,pixel48,pixel49,pixel50,pixel51,pixel52,pixel53,pixel54,pixel55,pixel56,pixel57,pixel58,pixel59,pixel60,pixel61,pixel62,pixel63,pixel64,pixel65,pixel66,pixel67,pixel68,pixel69,pixel70,pixel71,pixel72,pixel73,pixel74,pixel75,pixel76,pixel77,pixel78,pixel79,pixel80,pixel81,pixel82,pixel83,pi...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RFormulaModel(ResolvedRFormula(label=label, terms=[pixel0,pixel1,pixel2,pixel3,pixel4,pixel5,pixel6,pixel7,pixel8,pixel9,pixel10,pixel11,pixel12,pixel13,pixel14,pixel15,pixel16,pixel17,pixel18,pixel19,pixel20,pixel21,pixel22,pixel23,pixel24,pixel25,pixel26,pixel27,pixel28,pixel29,pixel30,pixel31,pixel32,pixel33,pixel34,pixel35,pixel36,pixel37,pixel38,pixel39,pixel40,pixel41,pixel42,pixel43,pixel44,pixel45,pixel46,pixel47,pixel48,pixel49,pixel50,pixel51,pixel52,pixel53,pixel54,pixel55,pixel56,pixel57,pixel58,pixel59,pixel60,pixel61,pixel62,pixel63,pixel64,pixel65,pixel66,pixel67,pixel68,pixel69,pixel70,pixel71,pixel72,pixel73,pixel74,pixel75,pixel76,pixel77,pixel78,pixel79,pixel80,pixel81,pixel82,pixel83,pi..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val formula = \"label ~ .\"\n",
    "val rformula = new RFormula()\n",
    "  .setFormula(formula)\n",
    "  .fit(train)\n",
    "\n",
    "rformula.write.overwrite().save(PROTOCOL + MODEL_DIR + \"/kaggle_digit_recognizer_rformula.mdl\")\n",
    "\n",
    "val trainDataDF = timed(\n",
    "    \"RFormula\",\n",
    "    rformula\n",
    "        .transform(train)\n",
    "        .select(\"label\", \"features\")\n",
    ")\n",
    "\n",
    "println(timing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    1|(784,[132,133,134...|\n",
      "|    0|(784,[122,123,124...|\n",
      "|    1|(784,[124,125,126...|\n",
      "|    4|(784,[146,147,148...|\n",
      "|    0|(784,[121,122,123...|\n",
      "|    0|(784,[124,125,126...|\n",
      "|    7|(784,[202,203,204...|\n",
      "|    3|(784,[177,178,179...|\n",
      "|    5|(784,[153,154,155...|\n",
      "|    3|(784,[119,120,121...|\n",
      "|    8|(784,[180,181,182...|\n",
      "|    9|(784,[182,183,184...|\n",
      "|    1|(784,[154,155,156...|\n",
      "|    3|(784,[144,145,146...|\n",
      "|    3|(784,[122,123,124...|\n",
      "|    1|(784,[156,157,158...|\n",
      "|    2|(784,[148,149,150...|\n",
      "|    0|(784,[129,130,131...|\n",
      "|    7|(784,[206,207,208...|\n",
      "|    5|(784,[121,122,123...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDataDF.printSchema\n",
    "trainDataDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rowToVector: (r: org.apache.spark.sql.Row)org.apache.spark.mllib.linalg.Vector\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "def rowToVector(r: Row): org.apache.spark.mllib.linalg.Vector = {\n",
    "    val v = r.getAs[org.apache.spark.ml.linalg.Vector](0)\n",
    "    org.apache.spark.mllib.linalg.Vectors.fromML(v)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainDataMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@215c7180\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.RowMatrix@215c7180"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainDataMatrix = new RowMatrix(\n",
    "    trainDataDF.select(\"features\").rdd.map(rowToVector)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing RFormula took 811 ms.\n",
      "Processing SVD took 23324 ms.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "svd = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SingularValueDecomposition(org.apache.spark.mllib.linalg.distributed.RowMatrix@3d10b71e,[323503.703778161,109669.14717327854,101621.48745300347,93843.81264344015,87209.07876311651,78809.72055197941,72567.49168291429,64828.82470555653,64416.68658534496,58308.861751418466,55406.27915475731,54750.55160735113,49609.08790700611,49550.52462799281,47953.954194702324,46331.72758655017,44275.238616400704,43359.10907006242,41448.81945675147,40775.83066771042,39324.43779854911,38274.73752844696,37316.21577405969,36320.82031107616,35793.95790153957,34803.45941347596,34247.123321015686,33487.606203318144,32683.269877895247,31472.91710587883,30818.6...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val svd: SingularValueDecomposition[RowMatrix, Matrix] = timed(\n",
    "    \"SVD\",\n",
    "    trainDataMatrix.computeSVD(numFeatures, computeU = true)\n",
    ")\n",
    "println(timing)\n",
    "\n",
    "val U: RowMatrix = svd.U  // The U factor is a RowMatrix.\n",
    "val s: Vector = svd.s     // The singular values are stored in a local dense vector.\n",
    "val V: Matrix = svd.V     // The V factor is a local dense matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "singluarValues = Array(323503.703778161, 109669.14717327854, 101621.48745300347, 93843.81264344015, 87209.07876311651, 78809.72055197941, 72567.49168291429, 64828.82470555653, 64416.68658534496, 58308.861751418466, 55406.27915475731, 54750.55160735113, 49609.08790700611, 49550.52462799281, 47953.954194702324, 46331.72758655017, 44275.238616400704, 43359.10907006242, 41448.81945675147, 40775.83066771042, 39324.43779854911, 38274.73752844696, 37316.21577405969, 36320.82031107616, 35793.95790153957, 34803.45941347596, 34247.123321015686, 33487.606203318144, 32683.269877895247, 31472.91710587883, 30818.644127794658, 30355.606099707322, 29404.347846111687, 29255.758990244067, 28531.531896135904, 27953.24406512744, 27100.342265174775, 26516.189399254694, 26189.552346276807, 259...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(323503.703778161, 109669.14717327854, 101621.48745300347, 93843.81264344015, 87209.07876311651, 78809.72055197941, 72567.49168291429, 64828.82470555653, 64416.68658534496, 58308.861751418466, 55406.27915475731, 54750.55160735113, 49609.08790700611, 49550.52462799281, 47953.954194702324, 46331.72758655017, 44275.238616400704, 43359.10907006242, 41448.81945675147, 40775.83066771042, 39324.43779854911, 38274.73752844696, 37316.21577405969, 36320.82031107616, 35793.95790153957, 34803.45941347596, 34247.123321015686, 33487.606203318144, 32683.269877895247, 31472.91710587883, 30818.644127794658, 30355.606099707322, 29404.347846111687, 29255.758990244067, 28531.531896135904, 27953.24406512744, 27100.342265174775, 26516.189399254694, 26189.552346276807, 259..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val singluarValues = s.toDense.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of principal components (k) to cover COVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "windowSpec = org.apache.spark.sql.expressions.WindowSpec@70163cb0\n",
       "windowSpecAll = org.apache.spark.sql.expressions.WindowSpec@2451d8a8\n",
       "coverageDF = [id: int, singluar_value: double ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, singluar_value: double ... 3 more fields]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val windowSpec = Window\n",
    "  .orderBy(desc(\"singluar_value\"))\n",
    "  .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "  \n",
    "val windowSpecAll = Window\n",
    "  .orderBy(desc(\"singluar_value\"))\n",
    "  .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "val coverageDF = sc.parallelize(singluarValues).toDF(\"singluar_value\")\n",
    "    .withColumn(\n",
    "        \"id\",\n",
    "        row_number().over(windowSpec)\n",
    "    )\n",
    "    .select(\"id\", \"singluar_value\")\n",
    "    .withColumn(\n",
    "        \"eignvalue\", \n",
    "        pow(col(\"singluar_value\"), 2) / lit(numSamples -1)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"cumulativeSum\", \n",
    "        sum(col(\"eignvalue\")).over(windowSpec)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"coverage\", \n",
    "        col(\"cumulativeSum\") / last(col(\"cumulativeSum\")).over(windowSpecAll)\n",
    "    )\n",
    "//coverageDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------+-----------------+------------------+\n",
      "| id|    singluar_value|         eignvalue|    cumulativeSum|          coverage|\n",
      "+---+------------------+------------------+-----------------+------------------+\n",
      "|102|11675.026832748754|3245.4642145147122|5443028.651091638|0.9501982263075994|\n",
      "+---+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kDF = [id: int, singluar_value: double ... 3 more fields]\n",
       "k = 102\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kDF = coverageDF.where(col(\"coverage\") >= lit(COVERAGE)).limit(1)\n",
    "kDF.show\n",
    "\n",
    "val k = kDF.select(\"id\").collect()(0).getInt(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pca = pca_63da76466e29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pca_63da76466e29"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pca = new PCA()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"pcaFeatures\")\n",
    "  .setK(k)\n",
    "  .fit(trainDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.write.overwrite().save(PROTOCOL + MODEL_DIR + \"/kaggle_digit_recognizer_pca.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputCol: input column name (current: features)\n",
       "k: the number of principal components (> 0) (current: 102)\n",
       "outputCol: output column name (default: pca_63da76466e29__output, current: pcaFeatures)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09748937689497461,0.07160266275027234,0.06145903355957282,0.05379301996327574,0.04894262134042076,0.04303213992139485,0.03277050763580546,0.028921031736972334,0.02766902346628119,0.02348871029531937,0.02099325425271383,0.020590011597844095,0.017025534993579613,0.01692787021349282,0.015811264063787012,0.01483239617649628,0.013196878946996466,0.012827270808166676,0.01187976141356012,0.011527547329596514,0.010721912242319678,0.010151993021548662,0.00964902259415799,0.009128460679541317,0.008876408591128955,0.008387663079557076,0.008118558545110089,0.007774057474595946,0.007406351163183782,0.006866614888087311,0.006579822105222276,0.0063879861144869815,0.005993670159253401,0.005889134101342426,0.005643351784057687,0.005409670481475439,0.005..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explainedVariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9169591926906049"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explainedVariance.values.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pcaTrainDataDF = [label: int, features: vector ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: int, features: vector ... 1 more field]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pcaTrainDataDF = pca.transform(trainDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- pcaFeatures: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pcaTrainDataDF.printSchema\n",
    "//pcaTrainDataDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf = rfc_ea4a7195cd70\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. (default: false)\n",
       "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext (default: 10)\n",
       "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all,...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
    "val rf = new RandomForestClassifier()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"pcaFeatures\")\n",
    "  .setFeatureSubsetStrategy(\"auto\")\n",
    "\n",
    "rf.explainParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator = mcEval_af99de95a8a5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "mcEval_af99de95a8a5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "    .setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "params = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "\trfc_ea4a7195cd70-maxDepth: 5,\n",
       "\trfc_ea4a7195cd70-numTrees: 50,\n",
       "\trfc_ea4a7195cd70-subsamplingRate: 0.1\n",
       "}, {\n",
       "\trfc_ea4a7195cd70-maxDepth: 5,\n",
       "\trfc_ea4a7195cd70-numTrees: 100,\n",
       "\trfc_ea4a7195cd70-subsamplingRate: 0.1\n",
       "}, {\n",
       "\trfc_ea4a7195cd70-maxDepth: 10,\n",
       "\trfc_ea4a7195cd70-numTrees: 50,\n",
       "\trfc_ea4a7195cd70-subsamplingRate: 0.1\n",
       "}, {\n",
       "\trfc_ea4a7195cd70-maxDepth: 10,\n",
       "\trfc_ea4a7195cd70-numTrees: 100,\n",
       "\trfc_ea4a7195cd70-subsamplingRate: 0.1\n",
       "}, {\n",
       "\trfc_ea4a7195cd70-maxDepth: 15,\n",
       "\trfc_ea4a7195cd70-numTrees: 50,\n",
       "\trfc_ea4a7195cd70-subsamplingRate: 0.1\n",
       "}, {\n",
       "\trfc_ea4a7195cd70-maxDepth: 15,\n",
       "\trfc_ea4a7195cd70-numTrees: 100,\n",
       "\trfc_ea4a7195cd70-subsamplingRate: 0.1\n",
       "}, {\n",
       "\trfc_ea4a7195cd70-maxDepth: 5,\n",
       "\trfc_e...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "val params = new ParamGridBuilder()\n",
    "    .addGrid(rf.maxDepth, Array(10, 15, 20))\n",
    "    .addGrid(rf.numTrees, Array(100, 150))\n",
    "    .addGrid(rf.subsamplingRate, Array(0.3, 0.5, 0.7))\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Cross Validation](https://spark.apache.org/docs/latest/ml-tuning.html#cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel}\n",
    "val cv = new TrainValidationSplit()\n",
    "  .setEstimatorParamMaps(params)\n",
    "  .setEstimator(rf)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setNumFolds(CROSS_VALIDATE_NUM_HOLDS)  // Use 3+ in practice\n",
    "  .setParallelism(CROSS_VALIDATE_PARALLELISM)  // Evaluate up to 2 parameter settings in parallel\n",
    "\n",
    "val cvFitted = timed(\n",
    "    \"Cross validation grid search\",\n",
    "    cv.fit(pcaTrainDataDF)\n",
    ")\n",
    "\n",
    "cvFitted.write.overwrite().save(PROTOCOL + MODEL_DIR + \"/kaggle_digit_recognizer_cv.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val fitted = cvFitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Train Validate Split](https://spark.apache.org/docs/latest/ml-tuning.html#train-validation-split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvsFitted = tvs_c6aadc18f7f1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tvs_c6aadc18f7f1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.{TrainValidationSplit, TrainValidationSplitModel}\n",
    "val tvs = new TrainValidationSplit()\n",
    "  .setTrainRatio(0.75) // also the default.\n",
    "  .setEstimatorParamMaps(params)\n",
    "  .setEstimator(rf)\n",
    "  .setEvaluator(evaluator)\n",
    "\n",
    "val tvsFitted = timed(\n",
    "    \"Train validation split grid search\",\n",
    "    tvs.fit(pcaTrainDataDF)\n",
    ")\n",
    "\n",
    "tvsFitted.write.overwrite().save(PROTOCOL + MODEL_DIR + \"/kaggle_digit_recognizer_tvs.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val fitted = tvsFitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing RFormula took 740 ms.\n",
      "Processing SVD took 25087 ms.\n",
      "Processing GridSearch took 931541 ms.\n",
      "\n",
      "max depth 15\n",
      "num trees 100\n",
      "impurity  gini\n",
      "subsamplingRate rfc_ea4a7195cd70__subsamplingRate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainedModel = RandomForestClassificationModel (uid=rfc_ea4a7195cd70) with 100 trees\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassificationModel (uid=rfc_ea4a7195cd70) with 100 trees"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = fitted.bestModel.asInstanceOf[RandomForestClassificationModel]\n",
    "model.explainParams()\n",
    "println(timing)\n",
    "println(s\"max depth ${model.getMaxDepth}\")\n",
    "println(s\"num trees ${model.getNumTrees}\")\n",
    "println(s\"impurity  ${model.getImpurity}\")\n",
    "println(s\"subsamplingRate ${model.getSubsamplingRate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write.overwrite().save(PROTOCOL + MODEL_DIR + \"/kaggle_digit_recognizer_rf.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions = [label: int, features: vector ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: int, features: vector ... 4 more fields]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = fitted.transform(pcaTrainDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|         pcaFeatures|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    1|(784,[132,133,134...|[103.738813757989...|[0.00450894878577...|[4.50894878577726...|       1.0|\n",
      "|    0|(784,[122,123,124...|[2466.78627830941...|[84.5164983198871...|[0.84516498319887...|       0.0|\n",
      "|    1|(784,[124,125,126...|[-121.55984060477...|[0.03951360973791...|[3.95136097379186...|       1.0|\n",
      "|    4|(784,[146,147,148...|[599.578991071955...|[1.39702870431335...|[0.01397028704313...|       4.0|\n",
      "|    0|(784,[121,122,123...|[2689.04430947599...|[91.5365703890843...|[0.91536570389084...|       0.0|\n",
      "|    0|(784,[124,125,126...|[1253.08650413365...|[50.3024157705876...|[0.50302415770587...|       0.0|\n",
      "|    7|(784,[202,203,204...|[93.0114290617944...|[0.03352711302764...|[3.35271130276407...|       7.0|\n",
      "|    3|(784,[177,178,179...|[650.952778816167...|[1.73961193395070...|[0.01739611933950...|       3.0|\n",
      "|    5|(784,[153,154,155...|[1115.56395904828...|[6.37125084334166...|[0.06371250843341...|       5.0|\n",
      "|    3|(784,[119,120,121...|[1062.72668192117...|[5.39886708465388...|[0.05398867084653...|       3.0|\n",
      "|    8|(784,[180,181,182...|[1029.01690081557...|[1.24305645817954...|[0.01243056458179...|       8.0|\n",
      "|    9|(784,[182,183,184...|[458.805321389772...|[0.38137912565999...|[0.00381379125659...|       9.0|\n",
      "|    1|(784,[154,155,156...|[-200.34133976161...|[0.03905056059882...|[3.90505605988214...|       1.0|\n",
      "|    3|(784,[144,145,146...|[751.26392695719,...|[3.36332391708723...|[0.03363323917087...|       3.0|\n",
      "|    3|(784,[122,123,124...|[1265.44211418056...|[1.12930338400595...|[0.01129303384005...|       3.0|\n",
      "|    1|(784,[156,157,158...|[-199.11010313255...|[2.31286142839020...|[0.02312861428390...|       1.0|\n",
      "|    2|(784,[148,149,150...|[762.715694923052...|[4.02829922105445...|[0.04028299221054...|       2.0|\n",
      "|    0|(784,[129,130,131...|[1744.79986516160...|[99.0385251111292...|[0.99038525111129...|       0.0|\n",
      "|    7|(784,[206,207,208...|[128.314928856542...|[2.10159687248402...|[0.02101596872484...|       7.0|\n",
      "|    5|(784,[121,122,123...|[1731.44148649030...|[8.80220052305418...|[0.08802200523054...|       5.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.005428571428571449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy = 0.9945714285714286\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9945714285714286"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accuracy = fitted.getEvaluator.evaluate(predictions)\n",
    "println(s\"Test Error = ${(1.0 - accuracy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test = [pixel0: int, pixel1: int ... 782 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[pixel0: int, pixel1: int ... 782 more fields]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test = spark.read.format(\"csv\")\n",
    "    .option(\"header\", true)\n",
    "    .option(\"inferSchema\", true)\n",
    "    .load(PROTOCOL + DATA_DIR + \"/test.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testDataDF = [features: vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: vector]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testDataDF = rformula\n",
    "    .transform(test)\n",
    "    .select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(784,[122,123,124...|\n",
      "|(784,[179,180,181...|\n",
      "|(784,[181,182,183...|\n",
      "|(784,[208,210,211...|\n",
      "|(784,[95,96,97,98...|\n",
      "|(784,[202,203,204...|\n",
      "|(784,[124,125,126...|\n",
      "|(784,[122,123,124...|\n",
      "|(784,[153,154,155...|\n",
      "|(784,[121,122,123...|\n",
      "|(784,[155,156,157...|\n",
      "|(784,[179,180,181...|\n",
      "|(784,[147,148,149...|\n",
      "|(784,[126,127,128...|\n",
      "|(784,[177,178,204...|\n",
      "|(784,[120,121,122...|\n",
      "|(784,[119,120,121...|\n",
      "|(784,[125,126,153...|\n",
      "|(784,[179,180,181...|\n",
      "|(784,[123,124,125...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDataDF\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pcaTestDataDF = [features: vector, pcaFeatures: vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: vector, pcaFeatures: vector]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pcaTestDataDF = pca.transform(testDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- pcaFeatures: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pcaTestDataDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|         pcaFeatures|\n",
      "+--------------------+--------------------+\n",
      "|(784,[122,123,124...|[1612.42042408780...|\n",
      "|(784,[179,180,181...|[1919.58554625988...|\n",
      "|(784,[181,182,183...|[258.514350043858...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pcaTestDataDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "validations = [features: vector, pcaFeatures: vector ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: vector, pcaFeatures: vector ... 3 more fields]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val validations = fitted.transform(pcaTestDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|            features|         pcaFeatures|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|(784,[122,123,124...|[1612.42042408780...|[3.10440378782087...|[0.03104403787820...|       2.0|\n",
      "|(784,[179,180,181...|[1919.58554625988...|[89.8726237980590...|[0.89872623798059...|       0.0|\n",
      "|(784,[181,182,183...|[258.514350043858...|[3.36094066825166...|[0.03360940668251...|       9.0|\n",
      "|(784,[208,210,211...|[497.736897425884...|[7.46759295209640...|[0.07467592952096...|       4.0|\n",
      "|(784,[95,96,97,98...|[667.552229232708...|[2.55710185908117...|[0.02557101859081...|       3.0|\n",
      "|(784,[202,203,204...|[487.203794732073...|[2.54427274091337...|[0.02544272740913...|       7.0|\n",
      "|(784,[124,125,126...|[2481.24718102618...|[82.1917772496175...|[0.82191777249617...|       0.0|\n",
      "|(784,[122,123,124...|[1010.70828480165...|[8.84884446262091...|[0.08848844462620...|       3.0|\n",
      "|(784,[153,154,155...|[1997.83281735405...|[92.9119416400884...|[0.92911941640088...|       0.0|\n",
      "|(784,[121,122,123...|[815.544101062854...|[0.49995932007300...|[0.00499959320073...|       3.0|\n",
      "|(784,[155,156,157...|[1040.42285051346...|[0.38775270601846...|[0.00387752706018...|       5.0|\n",
      "|(784,[179,180,181...|[348.064542982559...|[3.38012863659269...|[0.03380128636592...|       7.0|\n",
      "|(784,[147,148,149...|[609.001195958960...|[0.09115815691158...|[9.11581569115815...|       4.0|\n",
      "|(784,[126,127,128...|[2378.43162566382...|[91.3520568777765...|[0.91352056877776...|       0.0|\n",
      "|(784,[177,178,204...|[807.580325397994...|[1.02454080096356...|[0.01024540800963...|       4.0|\n",
      "|(784,[120,121,122...|[1948.46110672588...|[10.1809588520756...|[0.10180958852075...|       3.0|\n",
      "|(784,[119,120,121...|[938.449458365679...|[1.21642039684859...|[0.01216420396848...|       3.0|\n",
      "|(784,[125,126,153...|[-53.747515943519...|[0.10161579174312...|[0.00101615791743...|       1.0|\n",
      "|(784,[179,180,181...|[476.763591666676...|[0.07801212427380...|[7.80121242738065...|       9.0|\n",
      "|(784,[123,124,125...|[1928.31618068484...|[92.9221191245062...|[0.92922119124506...|       0.0|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validations.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recgnizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification._\n",
    "\n",
    "import org.apache.spark.ml.linalg._\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.linalg.distributed._\n",
    "\n",
    "\n",
    "import java.time.{Period, LocalDate, Instant}\n",
    "import java.sql.Timestamp\n",
    "import java.time.temporal.ChronoUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- To left align the HTML components in Markdown -->\n",
       "<style>\n",
       "table {float:left}\n",
       "</style>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html\n",
    "<!-- To left align the HTML components in Markdown -->\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:71: error: stable identifier required, but this.$line7$read.spark.implicits found.\n",
       "       import spark.implicits._\n",
       "                    ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NUM_CORES = 4\n",
    "val NUM_PARTITIONS = 4\n",
    "\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "/*\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.driver.memory\", \"6g\")\n",
    "spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "spark.conf.set(\"spark.master\", \"spark://masa:7077\")\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.serializer,org.apache.spark.serializer.KryoSerializer)\n",
      "(spark.driver.host,10.138.111.209)\n",
      "(spark.eventLog.enabled,true)\n",
      "(spark.driver.port,36869)\n",
      "(spark.hadoop.validateOutputSpecs,True)\n",
      "(spark.repl.class.uri,spark://10.138.111.209:36869/classes)\n",
      "(spark.jars,file:/home/oonisim/.local/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.3.0-incubating.jar)\n",
      "(spark.repl.class.outputDir,/tmp/spark-2b364b40-7f3e-430d-9553-d54c5d78e599/repl-03a7e26f-3e7c-4606-bf3f-7dd89ec1460b)\n",
      "(spark.app.name,Apache Toree)\n",
      "(spark.driver.memory,2g)\n",
      "(spark.executor.instances,2)\n",
      "(spark.history.fs.logdirectory,hdfs://oonisim:8020/logs_spark)\n",
      "(spark.executor.id,driver)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.master,yarn)\n",
      "(spark.ui.filters,org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter)\n",
      "(spark.executor.memory,4g)\n",
      "(spark.eventLog.dir,hdfs://oonisim:8020/logs_spark)\n",
      "(spark.executor.cores,4)\n",
      "(spark.driver.appUIAddress,http://10.138.111.209:4041)\n",
      "(spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS,oonisim)\n",
      "(spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES,http://oonisim:8088/proxy/application_1577174691688_0003)\n",
      "(spark.app.id,application_1577174691688_0003)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "configMap: Unit = ()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val configMap = spark.conf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROTOCOL = hdfs://\n",
       "DATA_DIR = /data/kaggle/mnist\n",
       "RESULT_DIR = .\n",
       "COVERAGE = 0.95\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val PROTOCOL=\"hdfs://\"\n",
    "val DATA_DIR=\"/data/kaggle/mnist\"\n",
    "val RESULT_DIR=\".\"\n",
    "\n",
    "val COVERAGE:Double = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train = [label: int, pixel0: int ... 783 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: int, pixel0: int ... 783 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val train = spark.read.format(\"csv\")\n",
    "    .option(\"header\", true)\n",
    "    .option(\"inferSchema\", true)\n",
    "    .load(PROTOCOL + DATA_DIR + \"/train.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numFeatures = 784\n",
       "numSamples = 42000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numFeatures = train.columns.size - 1  // Remove \"label\" column\n",
    "val numSamples = train.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe for ML training\n",
    "\n",
    "Use R formula transformation to create label and features columns for Spark ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "formula = label ~ .\n",
       "rformula = RFormulaModel(ResolvedRFormula(label=label, terms=[pixel0,pixel1,pixel2,pixel3,pixel4,pixel5,pixel6,pixel7,pixel8,pixel9,pixel10,pixel11,pixel12,pixel13,pixel14,pixel15,pixel16,pixel17,pixel18,pixel19,pixel20,pixel21,pixel22,pixel23,pixel24,pixel25,pixel26,pixel27,pixel28,pixel29,pixel30,pixel31,pixel32,pixel33,pixel34,pixel35,pixel36,pixel37,pixel38,pixel39,pixel40,pixel41,pixel42,pixel43,pixel44,pixel45,pixel46,pixel47,pixel48,pixel49,pixel50,pixel51,pixel52,pixel53,pixel54,pixel55,pixel56,pixel57,pixel58,pixel59,pixel60,pixel61,pixel62,pixel63,pixel64,pixel65,pixel66,pixel67,pixel68,pixel69,pixel70,pixel71,pixel72,pixel73,pixel74,pixel75,pixel76,pixel77,pixel78,pixel79,pixel80,pixel81,pixel82,pixel83,pi...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RFormulaModel(ResolvedRFormula(label=label, terms=[pixel0,pixel1,pixel2,pixel3,pixel4,pixel5,pixel6,pixel7,pixel8,pixel9,pixel10,pixel11,pixel12,pixel13,pixel14,pixel15,pixel16,pixel17,pixel18,pixel19,pixel20,pixel21,pixel22,pixel23,pixel24,pixel25,pixel26,pixel27,pixel28,pixel29,pixel30,pixel31,pixel32,pixel33,pixel34,pixel35,pixel36,pixel37,pixel38,pixel39,pixel40,pixel41,pixel42,pixel43,pixel44,pixel45,pixel46,pixel47,pixel48,pixel49,pixel50,pixel51,pixel52,pixel53,pixel54,pixel55,pixel56,pixel57,pixel58,pixel59,pixel60,pixel61,pixel62,pixel63,pixel64,pixel65,pixel66,pixel67,pixel68,pixel69,pixel70,pixel71,pixel72,pixel73,pixel74,pixel75,pixel76,pixel77,pixel78,pixel79,pixel80,pixel81,pixel82,pixel83,pi..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val formula = \"label ~ .\"\n",
    "val rformula = new RFormula()\n",
    "  .setFormula(formula)\n",
    "  .fit(train)\n",
    "\n",
    "val trainDataDF = rformula\n",
    "    .transform(train)\n",
    "    .select(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDataDF.printSchema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    1|(784,[132,133,134...|\n",
      "|    0|(784,[122,123,124...|\n",
      "|    1|(784,[124,125,126...|\n",
      "|    4|(784,[146,147,148...|\n",
      "|    0|(784,[121,122,123...|\n",
      "|    0|(784,[124,125,126...|\n",
      "|    7|(784,[202,203,204...|\n",
      "|    3|(784,[177,178,179...|\n",
      "|    5|(784,[153,154,155...|\n",
      "|    3|(784,[119,120,121...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDataDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rowToVector: (r: org.apache.spark.sql.Row)org.apache.spark.mllib.linalg.Vector\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "def rowToVector(r: Row): org.apache.spark.mllib.linalg.Vector = {\n",
    "    val v = r.getAs[org.apache.spark.ml.linalg.Vector](0)\n",
    "    org.apache.spark.mllib.linalg.Vectors.fromML(v)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainDataMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@48a89061\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.RowMatrix@48a89061"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainDataMatrix = new RowMatrix(\n",
    "    trainDataDF.select(\"features\").rdd.map(rowToVector)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svd = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SingularValueDecomposition(org.apache.spark.mllib.linalg.distributed.RowMatrix@2f47a2e8,[323503.703778161,109669.14717327854,101621.48745300347,93843.81264344015,87209.07876311651,78809.72055197941,72567.49168291429,64828.82470555653,64416.68658534496,58308.861751418466,55406.27915475731,54750.55160735113,49609.08790700611,49550.52462799281,47953.954194702324,46331.72758655017,44275.238616400704,43359.10907006242,41448.81945675147,40775.83066771042,39324.43779854911,38274.73752844696,37316.21577405969,36320.82031107616,35793.95790153957,34803.45941347596,34247.123321015686,33487.606203318144,32683.269877895247,31472.91710587883,30818.6...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val svd: SingularValueDecomposition[RowMatrix, Matrix] = trainDataMatrix.computeSVD(numFeatures, computeU = true)\n",
    "val U: RowMatrix = svd.U  // The U factor is a RowMatrix.\n",
    "val s: Vector = svd.s     // The singular values are stored in a local dense vector.\n",
    "val V: Matrix = svd.V     // The V factor is a local dense matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "singluarValues = Array(323503.703778161, 109669.14717327854, 101621.48745300347, 93843.81264344015, 87209.07876311651, 78809.72055197941, 72567.49168291429, 64828.82470555653, 64416.68658534496, 58308.861751418466, 55406.27915475731, 54750.55160735113, 49609.08790700611, 49550.52462799281, 47953.954194702324, 46331.72758655017, 44275.238616400704, 43359.10907006242, 41448.81945675147, 40775.83066771042, 39324.43779854911, 38274.73752844696, 37316.21577405969, 36320.82031107616, 35793.95790153957, 34803.45941347596, 34247.123321015686, 33487.606203318144, 32683.269877895247, 31472.91710587883, 30818.644127794658, 30355.606099707322, 29404.347846111687, 29255.758990244067, 28531.531896135904, 27953.24406512744, 27100.342265174775, 26516.189399254694, 26189.552346276807, 259...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(323503.703778161, 109669.14717327854, 101621.48745300347, 93843.81264344015, 87209.07876311651, 78809.72055197941, 72567.49168291429, 64828.82470555653, 64416.68658534496, 58308.861751418466, 55406.27915475731, 54750.55160735113, 49609.08790700611, 49550.52462799281, 47953.954194702324, 46331.72758655017, 44275.238616400704, 43359.10907006242, 41448.81945675147, 40775.83066771042, 39324.43779854911, 38274.73752844696, 37316.21577405969, 36320.82031107616, 35793.95790153957, 34803.45941347596, 34247.123321015686, 33487.606203318144, 32683.269877895247, 31472.91710587883, 30818.644127794658, 30355.606099707322, 29404.347846111687, 29255.758990244067, 28531.531896135904, 27953.24406512744, 27100.342265174775, 26516.189399254694, 26189.552346276807, 259..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val singluarValues = s.toDense.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of principal components (k) to cover COVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------+------------------+-------------------+\n",
      "| id|    singluar_value|         eignvalue|     cumulativeSum|           coverage|\n",
      "+---+------------------+------------------+------------------+-------------------+\n",
      "|  1|  323503.703778161| 2491836.623685996| 2491836.623685996|0.43500390900934977|\n",
      "|  2|109669.14717327854| 286371.6241271037|   2778208.2478131|0.48499626193511053|\n",
      "|  3|101621.48745300347|245885.06183863763|3024093.3096517376| 0.5279208108602296|\n",
      "|  4| 93843.81264344015|209687.40140139285|  3233780.71105313| 0.5645262762477199|\n",
      "|  5| 87209.07876311651|181085.82153649992|  3414866.53258963| 0.5961387180449769|\n",
      "+---+------------------+------------------+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "windowSpec = org.apache.spark.sql.expressions.WindowSpec@68c286f5\n",
       "windowSpecAll = org.apache.spark.sql.expressions.WindowSpec@364c16ba\n",
       "coverageDF = [id: int, singluar_value: double ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, singluar_value: double ... 3 more fields]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val windowSpec = Window\n",
    "  .orderBy(desc(\"singluar_value\"))\n",
    "  .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "  \n",
    "val windowSpecAll = Window\n",
    "  .orderBy(desc(\"singluar_value\"))\n",
    "  .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "val coverageDF = sc.parallelize(singluarValues).toDF(\"singluar_value\")\n",
    "    .withColumn(\n",
    "        \"id\",\n",
    "        row_number().over(windowSpec)\n",
    "    )\n",
    "    .select(\"id\", \"singluar_value\")\n",
    "    .withColumn(\n",
    "        \"eignvalue\", \n",
    "        pow(col(\"singluar_value\"), 2) / lit(numSamples -1)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"cumulativeSum\", \n",
    "        sum(col(\"eignvalue\")).over(windowSpec)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"coverage\", \n",
    "        col(\"cumulativeSum\") / last(col(\"cumulativeSum\")).over(windowSpecAll)\n",
    "    )\n",
    "coverageDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------+-----------------+------------------+\n",
      "| id|    singluar_value|         eignvalue|    cumulativeSum|          coverage|\n",
      "+---+------------------+------------------+-----------------+------------------+\n",
      "|102|11675.026832748754|3245.4642145147122|5443028.651091638|0.9501982263075994|\n",
      "+---+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kDF = [id: int, singluar_value: double ... 3 more fields]\n",
       "k = 102\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kDF = coverageDF.where(col(\"coverage\") >= lit(COVERAGE)).limit(1)\n",
    "kDF.show\n",
    "\n",
    "val k = kDF.select(\"id\").collect()(0).getInt(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pca = pca_755c79044108\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pca_755c79044108"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pca = new PCA()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"pcaFeatures\")\n",
    "  .setK(k)\n",
    "  .fit(trainDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputCol: input column name (current: features)\n",
       "k: the number of principal components (> 0) (current: 102)\n",
       "outputCol: output column name (default: pca_755c79044108__output, current: pcaFeatures)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09748937689497461,0.07160266275027234,0.06145903355957282,0.05379301996327574,0.04894262134042076,0.04303213992139485,0.03277050763580546,0.028921031736972334,0.02766902346628119,0.02348871029531937,0.02099325425271383,0.020590011597844095,0.017025534993579613,0.01692787021349282,0.015811264063787012,0.01483239617649628,0.013196878946996466,0.012827270808166676,0.01187976141356012,0.011527547329596514,0.010721912242319678,0.010151993021548662,0.00964902259415799,0.009128460679541317,0.008876408591128955,0.008387663079557076,0.008118558545110089,0.007774057474595946,0.007406351163183782,0.006866614888087311,0.006579822105222276,0.0063879861144869815,0.005993670159253401,0.005889134101342426,0.005643351784057687,0.005409670481475439,0.00..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explainedVariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9169591926906049"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explainedVariance.values.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pcaTrainDataDF = [label: int, features: vector ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: int, features: vector ... 1 more field]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pcaTrainDataDF = pca.transform(trainDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- pcaFeatures: vector (nullable = true)\n",
      "\n",
      "+-----+--------------------+--------------------+\n",
      "|label|            features|         pcaFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|    1|(784,[132,133,134...|[103.738813757989...|\n",
      "|    0|(784,[122,123,124...|[2466.78627830941...|\n",
      "|    1|(784,[124,125,126...|[-121.55984060477...|\n",
      "|    4|(784,[146,147,148...|[599.578991071955...|\n",
      "|    0|(784,[121,122,123...|[2689.04430947599...|\n",
      "|    0|(784,[124,125,126...|[1253.08650413365...|\n",
      "|    7|(784,[202,203,204...|[93.0114290617944...|\n",
      "|    3|(784,[177,178,179...|[650.952778816167...|\n",
      "|    5|(784,[153,154,155...|[1115.56395904828...|\n",
      "|    3|(784,[119,120,121...|[1062.72668192117...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pcaTrainDataDF.printSchema\n",
    "pcaTrainDataDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf = rfc_54f91692ea43\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n",
       "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. (default: false)\n",
       "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext (default: 10)\n",
       "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
    "val rf = new RandomForestClassifier()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"pcaFeatures\")\n",
    "  .setFeatureSubsetStrategy(\"auto\")\n",
    "\n",
    "rf.explainParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator = mcEval_7be655db0fab\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "mcEval_7be655db0fab"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "    .setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "params = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "\trfc_54f91692ea43-maxDepth: 3,\n",
       "\trfc_54f91692ea43-numTrees: 10,\n",
       "\trfc_54f91692ea43-subsamplingRate: 0.1\n",
       "}, {\n",
       "\trfc_54f91692ea43-maxDepth: 3,\n",
       "\trfc_54f91692ea43-numTrees: 10,\n",
       "\trfc_54f91692ea43-subsamplingRate: 0.5\n",
       "}, {\n",
       "\trfc_54f91692ea43-maxDepth: 3,\n",
       "\trfc_54f91692ea43-numTrees: 20,\n",
       "\trfc_54f91692ea43-subsamplingRate: 0.1\n",
       "}, {\n",
       "\trfc_54f91692ea43-maxDepth: 3,\n",
       "\trfc_54f91692ea43-numTrees: 20,\n",
       "\trfc_54f91692ea43-subsamplingRate: 0.5\n",
       "}, {\n",
       "\trfc_54f91692ea43-maxDepth: 5,\n",
       "\trfc_54f91692ea43-numTrees: 10,\n",
       "\trfc_54f91692ea43-subsamplingRate: 0.1\n",
       "}, {\n",
       "\trfc_54f91692ea43-maxDepth: 5,\n",
       "\trfc_54f91692ea43-numTrees: 10,\n",
       "\trfc_54f91692ea43-subsamplingRate: 0.5\n",
       "}, {\n",
       "\trfc_54f91692ea43-maxDepth: 5,\n",
       "\trfc_54f91692...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "val params = new ParamGridBuilder()\n",
    "    .addGrid(rf.maxDepth, Array(3, 5))\n",
    "    .addGrid(rf.numTrees, Array(10, 20))\n",
    "    .addGrid(rf.subsamplingRate, Array(0.1, 0.5))\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvs = tvs_55ae6576be9e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tvs_55ae6576be9e"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.{TrainValidationSplit, TrainValidationSplitModel}\n",
    "val tvs = new TrainValidationSplit()\n",
    "  .setTrainRatio(0.75) // also the default.\n",
    "  .setEstimatorParamMaps(params)\n",
    "  .setEstimator(rf)\n",
    "  .setEvaluator(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val tvsFitted = tvs.fit(pcaTrainDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvsFitted.write.overwrite().save(\"/data/model/rf.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvsFitterd.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
